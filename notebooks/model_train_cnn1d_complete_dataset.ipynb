{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be06af4-3903-48c4-a688-b15381e636d0",
   "metadata": {},
   "source": [
    "# CNN1D Model ‚Äî Emission Point Classification with Random Search and Cross-Validation\n",
    "\n",
    "This notebook develops a **1D Convolutional Neural Network (CNN)** to classify particle emission sources (E1, E2, E3) using flattened sensor windows as input.\n",
    "\n",
    "Unlike the MLP baseline, this model leverages **local temporal dependencies** across sensor readings using convolutional filters. To ensure robust evaluation and performance generalization, we apply:\n",
    "\n",
    "- **Random Search** over a defined space of hyperparameters;\n",
    "- **K-Fold Cross-Validation (K=5)** for fair performance estimation;\n",
    "- Final evaluation with test data using the best discovered configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "0. Import tensorflow and keras\n",
    "1. Check available GPU devices\n",
    "2. Confira CUDA support\n",
    "3. Print tensorflow version\n",
    "4. Check python version\n",
    "5. Import required libraries\n",
    "6. Load and preprocess dataset\n",
    "7. Define hyperparams search space\n",
    "8. Perform random search with K-fold Cross-Validation\n",
    "9. Select best hyperparameter configuration\n",
    "10. Final train/test split and reshape\n",
    "11. Build and train final model\n",
    "12. Evaluate final model on test set\n",
    "13. Save final model\n",
    "\n",
    "---\n",
    "\n",
    "> ‚úÖ This notebook introduces a more expressive model class than MLP by incorporating temporal patterns via CNN1D.  \n",
    "> ‚öôÔ∏è Combined with random search and cross-validation, this method offers a balance between speed and generalization for emission classification based on simulated sensor data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501e167-2fa0-4c17-962e-f80cb0c9aa56",
   "metadata": {},
   "source": [
    "## 0. Import TensorFlow and Keras\n",
    "\n",
    "We begin by importing TensorFlow and the Keras API, which will be used to build and train the CNN1D model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6438c2-12d5-4e2e-95ab-28c205327671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd781b3-0d89-4f7e-a461-5c6131f8b469",
   "metadata": {},
   "source": [
    "## 1. Check Available GPU Devices\n",
    "\n",
    "We verify whether a GPU is available for model training. This ensures that the model benefits from hardware acceleration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e86d299-b887-4574-955a-25b0b6e2f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of GPUs avaliable:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of GPUs avaliable: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc487cf4-308f-4472-aff9-9c58de41746c",
   "metadata": {},
   "source": [
    "## 2. Confirm CUDA Support\n",
    "\n",
    "This step checks if TensorFlow was built with CUDA GPU support, confirming compatibility for training on GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebd14c0-7253-435f-bd7c-8793165bbc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43959a50-d509-452a-ab4e-d0e2432a23f3",
   "metadata": {},
   "source": [
    "## 3. Print TensorFlow Version\n",
    "\n",
    "We print the TensorFlow version to ensure reproducibility and compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f90e815d-c0dd-4582-88c5-77e4fd1daa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57217b-8a9a-4c78-acdf-213c9b30ad4c",
   "metadata": {},
   "source": [
    "## 4. Check Python Version\n",
    "\n",
    "Verifying the Python version can help ensure compatibility with the notebook environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57fff01a-3eda-48f5-8a82-78bec9fca420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:53) [MSC v.1929 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb343a5-9e3e-42b4-a58f-bc56436154e6",
   "metadata": {},
   "source": [
    "## 5. Import Required Libraries\n",
    "\n",
    "We import all necessary libraries for data manipulation (NumPy, pandas), model building (Keras), evaluation, and preprocessing (scikit-learn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e37d17-474b-423f-a1af-e6bfdd234b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa74b56-0aa8-4857-9f02-1f3b98aaf0ab",
   "metadata": {},
   "source": [
    "## 6. Load and Preprocess Dataset\n",
    "\n",
    "We load the dataset from the processed directory and:\n",
    "- Remove the target column `Emission_Point` from features;\n",
    "- Convert all features to numeric types and fill any missing values;\n",
    "- Normalize the features using `StandardScaler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ef4c29-d28d-4fa8-bbec-960ebd2f3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/processed/complete_dataset.csv\")\n",
    "X = df.drop(columns=[\"Emission_Point\"])\n",
    "y = df[\"Emission_Point\"]\n",
    "\n",
    "# Convert to numeric and fill missing\n",
    "X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_cat = to_categorical(y_encoded)\n",
    "\n",
    "# Final shape for all inputs\n",
    "X_scaled = np.array(X_scaled)\n",
    "y_cat = np.array(y_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1eae93-4d2f-48a5-8ddc-d24904e43f2d",
   "metadata": {},
   "source": [
    "## 7. Define Hyperparameter Search Space\n",
    "\n",
    "We define the space for the random search over 3 hyperparameters:\n",
    "- `filters` (number of convolution filters),\n",
    "- `kernel` (size of the convolution window),\n",
    "- `dropout` (regularization strength).\n",
    "\n",
    "We also initialize the `KFold` strategy for 5-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c97691e6-f38f-479c-abbb-66d6d5e1e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space\n",
    "param_space = {\n",
    "    \"filters\": [32, 64, 128],\n",
    "    \"kernel\": [3, 5, 7],\n",
    "    \"dropout\": [0.2, 0.3, 0.4]\n",
    "}\n",
    "n_samples = 5  # Number of random configs to try\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd17ac1-d8a4-44b4-96bb-c622e0f29cc2",
   "metadata": {},
   "source": [
    "## 8. Perform Random Search with K-Fold Cross-Validation\n",
    "\n",
    "For each sampled configuration:\n",
    "- A CNN1D is built with the specified hyperparameters;\n",
    "- The model is trained and validated across 5 folds;\n",
    "- The mean validation accuracy is stored for later comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99cf64d4-2d2c-4070-9c2a-142fc32b8b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Testing configuration 1/5: {'filters': 64, 'kernel': 3, 'dropout': 0.2}\n",
      "‚úÖ Avg Accuracy: 0.8942\n",
      "\n",
      "üîß Testing configuration 2/5: {'filters': 64, 'kernel': 7, 'dropout': 0.3}\n",
      "‚úÖ Avg Accuracy: 0.8935\n",
      "\n",
      "üîß Testing configuration 3/5: {'filters': 128, 'kernel': 7, 'dropout': 0.2}\n",
      "‚úÖ Avg Accuracy: 0.8935\n",
      "\n",
      "üîß Testing configuration 4/5: {'filters': 128, 'kernel': 3, 'dropout': 0.3}\n",
      "‚úÖ Avg Accuracy: 0.8916\n",
      "\n",
      "üîß Testing configuration 5/5: {'filters': 64, 'kernel': 5, 'dropout': 0.4}\n",
      "‚úÖ Avg Accuracy: 0.8914\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_samples):\n",
    "    params = {\n",
    "        \"filters\": random.choice(param_space[\"filters\"]),\n",
    "        \"kernel\": random.choice(param_space[\"kernel\"]),\n",
    "        \"dropout\": random.choice(param_space[\"dropout\"])\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüîß Testing configuration {i+1}/{n_samples}: {params}\")\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "        y_train, y_val = y_cat[train_idx], y_cat[val_idx]\n",
    "\n",
    "        # Reshape for CNN1D\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "\n",
    "        clear_session()\n",
    "        model = Sequential([\n",
    "            Input(shape=(X_train.shape[1], 1)),\n",
    "            Conv1D(params[\"filters\"], kernel_size=params[\"kernel\"], activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(params[\"dropout\"]),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(3, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=5,\n",
    "                  batch_size=32,\n",
    "                  verbose=0,\n",
    "                  callbacks=[EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "\n",
    "        _, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        fold_accuracies.append(acc)\n",
    "\n",
    "    avg_acc = np.mean(fold_accuracies)\n",
    "    print(f\"‚úÖ Avg Accuracy: {avg_acc:.4f}\")\n",
    "    random_results.append((params, avg_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a6e15-bde4-4154-989e-a98cc03d3b52",
   "metadata": {},
   "source": [
    "## 9. Select Best Hyperparameter Configuration\n",
    "\n",
    "We identify and print the best-performing hyperparameter configuration based on average validation accuracy across folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b658d90-a32e-459f-8b39-92fb36f2953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Best config: {'filters': 64, 'kernel': 3, 'dropout': 0.2}\n",
      "üìà Best Validation Accuracy: 0.8942\n"
     ]
    }
   ],
   "source": [
    "best_config = sorted(random_results, key=lambda x: x[1], reverse=True)[0]\n",
    "print(f\"\\nüèÜ Best config: {best_config[0]}\")\n",
    "print(f\"üìà Best Validation Accuracy: {best_config[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b0d40-bbd4-4b13-a62f-ca74fc07544a",
   "metadata": {},
   "source": [
    "## 10. Final Train/Test Split and Reshape\n",
    "\n",
    "The entire dataset is split into 80% training and 20% testing.  \n",
    "Features are reshaped to match the CNN1D input format `(samples, time steps, channels)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9483820-da4b-46ae-a5a8-cd147fd58a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.3783 - accuracy: 0.8363 - val_loss: 0.2531 - val_accuracy: 0.8873\n",
      "Epoch 2/20\n",
      "5873/5873 [==============================] - 27s 5ms/step - loss: 0.2890 - accuracy: 0.8654 - val_loss: 0.2311 - val_accuracy: 0.8867\n",
      "Epoch 3/20\n",
      "5873/5873 [==============================] - 27s 5ms/step - loss: 0.2683 - accuracy: 0.8715 - val_loss: 0.2231 - val_accuracy: 0.8867\n",
      "Epoch 4/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2619 - accuracy: 0.8739 - val_loss: 0.2087 - val_accuracy: 0.8915\n",
      "Epoch 5/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2495 - accuracy: 0.8781 - val_loss: 0.2091 - val_accuracy: 0.8992\n",
      "Epoch 6/20\n",
      "5873/5873 [==============================] - 27s 5ms/step - loss: 0.2437 - accuracy: 0.8804 - val_loss: 0.2042 - val_accuracy: 0.8951\n",
      "Epoch 7/20\n",
      "5873/5873 [==============================] - 26s 5ms/step - loss: 0.2414 - accuracy: 0.8823 - val_loss: 0.2048 - val_accuracy: 0.8994\n",
      "Epoch 8/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2341 - accuracy: 0.8835 - val_loss: 0.2032 - val_accuracy: 0.8969\n",
      "Epoch 9/20\n",
      "5873/5873 [==============================] - 27s 5ms/step - loss: 0.2355 - accuracy: 0.8838 - val_loss: 0.2031 - val_accuracy: 0.8996\n",
      "Epoch 10/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2312 - accuracy: 0.8847 - val_loss: 0.2017 - val_accuracy: 0.9001\n",
      "Epoch 11/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2275 - accuracy: 0.8845 - val_loss: 0.1908 - val_accuracy: 0.8976\n",
      "Epoch 12/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2296 - accuracy: 0.8860 - val_loss: 0.1922 - val_accuracy: 0.9031\n",
      "Epoch 13/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2285 - accuracy: 0.8865 - val_loss: 0.1934 - val_accuracy: 0.9003\n",
      "Epoch 14/20\n",
      "5873/5873 [==============================] - 27s 5ms/step - loss: 0.2242 - accuracy: 0.8865 - val_loss: 0.1939 - val_accuracy: 0.9019\n",
      "Epoch 15/20\n",
      "5873/5873 [==============================] - 27s 5ms/step - loss: 0.2219 - accuracy: 0.8879 - val_loss: 0.2015 - val_accuracy: 0.8991\n",
      "Epoch 16/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2215 - accuracy: 0.8880 - val_loss: 0.1947 - val_accuracy: 0.9029\n",
      "Epoch 17/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2203 - accuracy: 0.8894 - val_loss: 0.1927 - val_accuracy: 0.9023\n",
      "Epoch 18/20\n",
      "5873/5873 [==============================] - 26s 4ms/step - loss: 0.2179 - accuracy: 0.8901 - val_loss: 0.1964 - val_accuracy: 0.9037\n"
     ]
    }
   ],
   "source": [
    "# Split full dataset for final evaluation\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_scaled, y_cat, test_size=0.2, stratify=y_cat, random_state=42)\n",
    "\n",
    "X_train_final = X_train_final.reshape(X_train_final.shape[0], X_train_final.shape[1], 1)\n",
    "X_test_final = X_test_final.reshape(X_test_final.shape[0], X_test_final.shape[1], 1)\n",
    "\n",
    "clear_session()\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train_final.shape[1], 1)),\n",
    "    Conv1D(best_config[0][\"filters\"], kernel_size=best_config[0][\"kernel\"], activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(best_config[0][\"dropout\"]),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_final, y_train_final,\n",
    "                    validation_data=(X_test_final, y_test_final),\n",
    "                    epochs=20, batch_size=32,\n",
    "                    callbacks=[EarlyStopping(patience=7, restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158c805-cbf0-4c06-a684-98b17027f2f5",
   "metadata": {},
   "source": [
    "## 11. Evaluate Final Model on Test Set\n",
    "\n",
    "We evaluate the final model using the test data and print:\n",
    "- Final test accuracy,\n",
    "- Full classification report,\n",
    "- Confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff899c19-9883-491b-a79e-f326e57b59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1469/1469 [==============================] - 4s 3ms/step - loss: 0.1908 - accuracy: 0.8976\n",
      "\n",
      "Final Test Accuracy: 0.8976\n",
      "1469/1469 [==============================] - 2s 1ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          E1       0.91      0.98      0.94     16231\n",
      "          E2       0.97      0.75      0.84     16233\n",
      "          E3       0.83      0.97      0.90     14520\n",
      "\n",
      "    accuracy                           0.90     46984\n",
      "   macro avg       0.90      0.90      0.89     46984\n",
      "weighted avg       0.91      0.90      0.89     46984\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15979   123   129]\n",
      " [ 1446 12115  2672]\n",
      " [  205   235 14080]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "loss, acc = model.evaluate(X_test_final, y_test_final)\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "y_pred = model.predict(X_test_final)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test_final, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=le.classes_))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_labels, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1202e3-d1ed-4954-aef0-3e13904df143",
   "metadata": {},
   "source": [
    "## 12. Save Final Model\n",
    "\n",
    "We save the final trained model to disk for future inference or deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437d62bb-09bf-4f5b-85fa-c1d86acf8821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final model saved as 'cnn1d_best_random_model.h5'\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../models/cnn1d_model_complete_dataset.h5\")\n",
    "print(\"‚úÖ Final model saved as '../models/cnn1d_model_complete_dataset.h5'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
