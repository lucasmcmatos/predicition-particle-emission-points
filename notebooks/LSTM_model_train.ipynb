{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b234d0b-8c94-4102-b580-69251f6e6d5c",
   "metadata": {},
   "source": [
    "# LSTM Model — Emission Point Classification\n",
    "\n",
    "This notebook develops a Long Short-Term Memory (LSTM) model for classifying the emission point of particles (E1, E2, E3) in a simulated industrial environment, based on sequential sensor readings. Unlike the MLP or CNN, the LSTM is designed to capture temporal dependencies in the data, making it especially suitable for time series forecasting and event detection.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Objectives\n",
    "\n",
    "- Build a deep learning model using LSTM layers to handle time-dependent input sequences;\n",
    "- Train the model on sensor windows generated from multiple simulation tags;\n",
    "- Apply proper scientific methodology to evaluate model performance, ensuring no data leakage between simulations;\n",
    "- Measure the model’s ability to generalize to unseen simulation conditions (10% holdout set);\n",
    "- Compare results with previous models (MLP and CNN) to guide future refinements.\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Data Structure and Considerations\n",
    "\n",
    "- **Input Data**: `complete_dataset.csv` — rows correspond to sensor readings with associated metadata.\n",
    "- **Sequence Generation**: Time windows (e.g., 30 time steps per sample) are created for each simulation (`tag`) independently.\n",
    "- **Normalization**: StandardScaler is applied **after** the data split to prevent leakage.\n",
    "- **Target Variable**: The emission class — `E1`, `E2`, or `E3`.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Notebook Structure\n",
    "\n",
    "1. Load Processed Data and Remove Constant Sensors  \n",
    "2. Generate LSTM-compatible Input Sequences  \n",
    "3. Perform Group-Based Train/Validation Split (by simulation `tag`)  \n",
    "4. Normalize Sensor Inputs  \n",
    "5. Define and Compile LSTM Model  \n",
    "6. Train Model with Callbacks and Class Weights  \n",
    "7. Evaluate on Validation Set  \n",
    "8. Final Generalization Test on 10% Unseen Data  \n",
    "9. Plot Metrics and Analyze Bias/Variance  \n",
    "10. Draw Scientific Conclusions\n",
    "\n",
    "---\n",
    "\n",
    "> ⚠️ All experiments follow good scientific practices, including group-wise validation to prevent overfitting on simulation-specific patterns, and final testing on unseen simulation tags for proper generalization assessment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f5e903-bb4e-4e27-ae31-af11d8e70ca3",
   "metadata": {},
   "source": [
    "### 0. Import Libraries and Configure GPU (if available)\n",
    "\n",
    "We begin by importing all the essential libraries for data processing, modeling, evaluation, and visualization. Additionally, we configure TensorFlow to use the GPU, which significantly accelerates training for deep learning models like LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6f53cd-8fec-476c-b373-7a951af18d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU detected and configured: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU config\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"✅ GPU detected and configured:\", gpus[0])\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e27cdd-3ce6-422f-aa00-1c37c4cb3651",
   "metadata": {},
   "source": [
    "### 1. Load Processed Dataset and Remove Constant Sensors\n",
    "\n",
    "In this step, we load the preprocessed sensor dataset and remove the constant sensors identified during the exploratory analysis. These sensors provide no variation and could negatively impact the learning process. We also separate a generalization subset (10% of the data) that will be evaluated only at the end to assess the true generalization capacity of the model on unseen simulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02442a5e-29e6-49f7-a60f-70af47ea1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load complete dataset\n",
    "df = pd.read_csv('../data/processed/dataset_timeseries.csv')\n",
    "\n",
    "# Load list of constant sensors and remove them\n",
    "with open('../data/processed/sensor_constantes.txt', 'r') as f:\n",
    "    constant_sensors = [line.strip() for line in f.readlines()]\n",
    "\n",
    "df = df.drop(columns=constant_sensors, errors='ignore')\n",
    "\n",
    "# Separate 10% of the dataset for final generalization testing (unseen simulations)\n",
    "df_full = df.copy()\n",
    "df_gen = df_full.sample(frac=0.10, random_state=42)\n",
    "df_modeling = df_full.drop(df_gen.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0164396-4684-4841-b3c4-f6e7b2d0d903",
   "metadata": {},
   "source": [
    "### 2. Generate Time-Based Input Sequences for LSTM\n",
    "\n",
    "LSTM models require input sequences with a fixed number of time steps. In this step, we generate sliding windows of sensor readings for each simulation (`tag`) independently. Each window corresponds to a temporal segment of the simulation and is labeled according to the emission class (`E1`, `E2`, `E3`).\n",
    "\n",
    "We use a window size of 30 time steps and a step size of 5 to generate overlapping windows. The function below ensures that data from different simulations is not mixed, preserving the integrity of each sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7626e419-f0ec-4bc3-8635-58f7684682e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_windows(df, window_size=30, step=5):\n",
    "    \"\"\"\n",
    "    Generate LSTM windows from the dataset.\n",
    "    Each window is a sequence of `window_size` time steps from the same simulation (tag).\n",
    "\n",
    "    Returns:\n",
    "        - X_seq: np.array of shape (n_windows, window_size, n_features)\n",
    "        - y_seq: np.array of corresponding labels\n",
    "        - tags: list of simulation tags (used for group-aware splitting)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    groups = []\n",
    "\n",
    "    for tag in df['tag'].unique():\n",
    "        group_data = df[df['tag'] == tag].sort_values('Time')\n",
    "        X = group_data.drop(columns=['classe', 'Altura', 'Time'], errors='ignore')\n",
    "        y = group_data['classe'].iloc[0]  # one label per simulation\n",
    "        n = len(X)\n",
    "\n",
    "        for i in range(0, n - window_size + 1, step):\n",
    "            window = X.iloc[i:i + window_size].values\n",
    "            if window.shape[0] == window_size:\n",
    "                sequences.append(window)\n",
    "                labels.append(y)\n",
    "                groups.append(tag)\n",
    "\n",
    "    return np.array(sequences), np.array(labels), np.array(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83a161-ca04-4aaa-b793-a621d212f0bc",
   "metadata": {},
   "source": [
    "### 3. Apply Sequence Generation and Encode Target Labels\n",
    "\n",
    "We now generate the sequential input data using the modeling dataset (`df_modeling`) and encode the categorical labels (`classe`) into numerical format. This prepares the data for training an LSTM model.\n",
    "\n",
    "The generated structure will be:\n",
    "- `X_seq`: input sequences of shape (n_windows, window_size, n_features)\n",
    "- `y_encoded`: encoded class labels (E1, E2, E3 → 0, 1, 2)\n",
    "- `tags`: simulation identifiers for group-aware validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "259f80fa-e376-4166-be80-383226818daf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate input sequences\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_seq, y_seq, tags \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_lstm_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_modeling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Encode target classes numerically\u001b[39;00m\n\u001b[0;32m      5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m, in \u001b[0;36mcreate_lstm_windows\u001b[1;34m(df, window_size, step)\u001b[0m\n\u001b[0;32m     12\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m groups \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique():\n\u001b[0;32m     16\u001b[0m     group_data \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m tag]\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     X \u001b[38;5;241m=\u001b[39m group_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAltura\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tag'"
     ]
    }
   ],
   "source": [
    "# Generate input sequences\n",
    "X_seq, y_seq, tags = create_lstm_windows(df_modeling, window_size=30, step=5)\n",
    "\n",
    "# Encode target classes numerically\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y_seq)\n",
    "\n",
    "# Check result shapes\n",
    "print(\"✅ Sequence shape:\", X_seq.shape)\n",
    "print(\"✅ Labels shape:\", y_encoded.shape)\n",
    "print(\"✅ Unique classes:\", encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45eb06-a10b-4b04-87fe-48ca90057e2a",
   "metadata": {},
   "source": [
    "### 4. Perform Group-Based Train/Validation Split\n",
    "\n",
    "To prevent data leakage and ensure reliable evaluation, we split the dataset into training and validation sets based on simulation tags (`tag`). This guarantees that the model is tested on simulations it has never seen before.\n",
    "\n",
    "We use `GroupShuffleSplit` to randomly assign 70% of the simulations to training and 30% to validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f133ab1-f9d2-4d7b-87b7-03797054bb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training set: (9786, 30, 322) labels: (9786,)\n",
      "✅ Validation set: (4368, 30, 322) labels: (4368,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Create splitter\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply split based on tags (simulations)\n",
    "for train_idx, val_idx in splitter.split(X_seq, y_encoded, groups=tags):\n",
    "    X_train, X_val = X_seq[train_idx], X_seq[val_idx]\n",
    "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "\n",
    "print(\"✅ Training set:\", X_train.shape, \"labels:\", y_train.shape)\n",
    "print(\"✅ Validation set:\", X_val.shape, \"labels:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5330a15-e2dc-4f93-b6b5-01e2f9a1351d",
   "metadata": {},
   "source": [
    "### 5. Normalize Input Sequences\n",
    "\n",
    "We normalize the sensor data using `StandardScaler`, fitting it only on the training set to avoid data leakage. The input sequences are reshaped before and after normalization to match the expected LSTM input format: `(samples, time_steps, features)`.\n",
    "\n",
    "This step ensures that all features contribute equally during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03300c99-a2d7-49b5-84c5-6d30390dd241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Normalization complete\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract sequence dimensions\n",
    "n_t, n_f = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "# Flatten for scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, n_f)).reshape(-1, n_t, n_f)\n",
    "X_val_scaled = scaler.transform(X_val.reshape(-1, n_f)).reshape(-1, n_t, n_f)\n",
    "\n",
    "# Cast to float32 for TensorFlow\n",
    "X_train_scaled = X_train_scaled.astype(np.float32)\n",
    "X_val_scaled = X_val_scaled.astype(np.float32)\n",
    "\n",
    "print(\"✅ Normalization complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a1eb4-bdcf-4bfa-8ddd-07e015e2e230",
   "metadata": {},
   "source": [
    "### 6. Define and Compile the LSTM Model\n",
    "\n",
    "We build a sequential LSTM model designed to learn temporal patterns from the sensor readings. The architecture includes two LSTM layers, dropout regularization, and batch normalization to improve generalization.\n",
    "\n",
    "The final layer is a dense softmax unit that outputs class probabilities for the three emission sources (E1, E2, E3).\n",
    "\n",
    "We compile the model using the Adam optimizer and sparse categorical cross-entropy loss, suitable for integer-labeled classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5a4ca26-b86c-4113-90a2-7a3f85ce5449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 30, 64)            99072     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,707\n",
      "Trainable params: 136,579\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, BatchNormalization\n",
    "\n",
    "# Define architecture\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(n_t, n_f)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(64),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b3f79-af24-4a32-9316-2926ba56cfb1",
   "metadata": {},
   "source": [
    "### 7. Train the LSTM Model with Progress Bar and Print Validation Metrics\n",
    "\n",
    "The model is trained with real-time progress feedback using `TqdmCallback`. After training, we evaluate the model’s classification performance on the validation set using multiple scientific metrics: accuracy, F1-score, precision, recall, and Cohen’s kappa.\n",
    "\n",
    "These metrics help assess both the general performance and the model’s ability to treat all classes fairly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5576928-1328-4e74-902a-dd0ef3ec2abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0epoch [00:00, ?epoch/s]\n",
      "  0%|                                                                                        | 0/30 [00:00<?, ?epoch/s]\n",
      "  0%|                                                                                    | 0.00/904 [00:00<?, ?batch/s]\u001b[A\n",
      "  0%|                                                | 1.00/904 [00:11<2:51:14, 11.4s/batch, loss=1.93, accuracy=0.125]\u001b[A\n",
      "  1%|▎                                                   | 5.00/904 [00:11<25:45, 1.72s/batch, loss=1.24, accuracy=0.1]\u001b[A\n",
      "  1%|▌                                                 | 10.0/904 [00:11<10:22, 1.44batch/s, loss=1.08, accuracy=0.112]\u001b[A\n",
      "  2%|▉                                                 | 17.0/904 [00:11<04:47, 3.09batch/s, loss=1.06, accuracy=0.153]\u001b[A\n",
      "  3%|█▍                                                | 26.0/904 [00:11<02:27, 5.97batch/s, loss=1.01, accuracy=0.204]\u001b[A\n",
      "  4%|█▉                                               | 35.0/904 [00:11<01:29, 9.74batch/s, loss=0.944, accuracy=0.238]\u001b[A\n",
      "  5%|██▍                                              | 44.0/904 [00:12<00:59, 14.5batch/s, loss=0.919, accuracy=0.272]\u001b[A\n",
      "  6%|██▉                                              | 54.0/904 [00:12<00:40, 21.1batch/s, loss=0.884, accuracy=0.304]\u001b[A\n",
      "  7%|███▍                                             | 63.0/904 [00:12<00:30, 27.8batch/s, loss=0.853, accuracy=0.324]\u001b[A\n",
      "  8%|███▉                                              | 72.0/904 [00:12<00:23, 35.2batch/s, loss=0.82, accuracy=0.351]\u001b[A\n",
      "  9%|████▍                                             | 81.0/904 [00:12<00:19, 43.0batch/s, loss=0.804, accuracy=0.38]\u001b[A\n",
      " 10%|████▉                                            | 90.0/904 [00:12<00:16, 50.7batch/s, loss=0.787, accuracy=0.405]\u001b[A\n",
      " 11%|█████▎                                           | 99.0/904 [00:12<00:14, 57.4batch/s, loss=0.773, accuracy=0.423]\u001b[A\n",
      " 12%|██████                                             | 108/904 [00:12<00:12, 64.1batch/s, loss=0.755, accuracy=0.44]\u001b[A\n",
      " 13%|██████▌                                           | 118/904 [00:12<00:11, 71.1batch/s, loss=0.738, accuracy=0.458]\u001b[A\n",
      " 14%|███████                                           | 127/904 [00:13<00:10, 74.6batch/s, loss=0.722, accuracy=0.475]\u001b[A\n",
      " 15%|███████▌                                          | 136/904 [00:13<00:09, 77.5batch/s, loss=0.709, accuracy=0.489]\u001b[A\n",
      " 16%|████████                                          | 146/904 [00:13<00:09, 80.8batch/s, loss=0.696, accuracy=0.504]\u001b[A\n",
      " 17%|████████▌                                         | 155/904 [00:13<00:09, 81.9batch/s, loss=0.689, accuracy=0.516]\u001b[A\n",
      " 18%|█████████▏                                        | 165/904 [00:13<00:08, 84.6batch/s, loss=0.684, accuracy=0.526]\u001b[A\n",
      " 19%|█████████▋                                        | 175/904 [00:13<00:08, 86.7batch/s, loss=0.671, accuracy=0.532]\u001b[A\n",
      " 20%|██████████▏                                       | 184/904 [00:13<00:08, 86.4batch/s, loss=0.657, accuracy=0.541]\u001b[A\n",
      " 21%|██████████▋                                       | 194/904 [00:13<00:08, 88.2batch/s, loss=0.645, accuracy=0.552]\u001b[A\n",
      " 22%|███████████▏                                      | 203/904 [00:13<00:07, 87.6batch/s, loss=0.638, accuracy=0.562]\u001b[A\n",
      " 23%|███████████▉                                       | 212/904 [00:13<00:08, 85.8batch/s, loss=0.628, accuracy=0.57]\u001b[A\n",
      " 24%|████████████▏                                     | 221/904 [00:14<00:08, 82.0batch/s, loss=0.622, accuracy=0.578]\u001b[A\n",
      " 25%|████████████▋                                     | 230/904 [00:14<00:08, 82.7batch/s, loss=0.613, accuracy=0.588]\u001b[A\n",
      " 26%|█████████████▏                                    | 239/904 [00:14<00:07, 83.5batch/s, loss=0.603, accuracy=0.597]\u001b[A\n",
      " 27%|█████████████▋                                    | 248/904 [00:14<00:07, 83.7batch/s, loss=0.594, accuracy=0.605]\u001b[A\n",
      " 28%|██████████████▏                                   | 257/904 [00:14<00:07, 81.5batch/s, loss=0.588, accuracy=0.611]\u001b[A\n",
      " 29%|██████████████▋                                   | 266/904 [00:14<00:07, 81.2batch/s, loss=0.582, accuracy=0.615]\u001b[A\n",
      " 30%|███████████████▏                                  | 275/904 [00:14<00:07, 80.1batch/s, loss=0.577, accuracy=0.621]\u001b[A\n",
      " 31%|███████████████▋                                  | 284/904 [00:14<00:07, 79.9batch/s, loss=0.571, accuracy=0.626]\u001b[A\n",
      " 32%|████████████████▏                                 | 292/904 [00:14<00:07, 78.5batch/s, loss=0.564, accuracy=0.631]\u001b[A\n",
      " 33%|████████████████▉                                  | 301/904 [00:15<00:07, 79.1batch/s, loss=0.56, accuracy=0.636]\u001b[A\n",
      " 34%|█████████████████▏                                | 310/904 [00:15<00:07, 81.9batch/s, loss=0.557, accuracy=0.639]\u001b[A\n",
      " 35%|█████████████████▉                                 | 319/904 [00:15<00:07, 83.0batch/s, loss=0.55, accuracy=0.643]\u001b[A\n",
      " 36%|██████████████████▏                               | 328/904 [00:15<00:06, 83.5batch/s, loss=0.544, accuracy=0.647]\u001b[A\n",
      " 37%|██████████████████▋                               | 337/904 [00:15<00:06, 85.1batch/s, loss=0.537, accuracy=0.652]\u001b[A\n",
      " 38%|███████████████████▏                              | 346/904 [00:15<00:06, 85.3batch/s, loss=0.532, accuracy=0.656]\u001b[A\n",
      " 39%|███████████████████▋                              | 355/904 [00:15<00:06, 84.5batch/s, loss=0.528, accuracy=0.659]\u001b[A\n",
      " 40%|████████████████████▏                             | 364/904 [00:15<00:06, 82.4batch/s, loss=0.524, accuracy=0.662]\u001b[A\n",
      " 41%|████████████████████▋                             | 373/904 [00:15<00:06, 80.9batch/s, loss=0.521, accuracy=0.666]\u001b[A\n",
      " 42%|█████████████████████▏                            | 382/904 [00:16<00:06, 80.7batch/s, loss=0.516, accuracy=0.669]\u001b[A\n",
      " 43%|█████████████████████▋                            | 391/904 [00:16<00:06, 82.9batch/s, loss=0.511, accuracy=0.673]\u001b[A\n",
      " 44%|██████████████████████▏                           | 401/904 [00:16<00:05, 85.5batch/s, loss=0.505, accuracy=0.676]\u001b[A\n",
      " 45%|██████████████████████▋                           | 410/904 [00:16<00:05, 86.2batch/s, loss=0.499, accuracy=0.681]\u001b[A\n",
      " 46%|███████████████████████▏                          | 419/904 [00:16<00:05, 86.1batch/s, loss=0.495, accuracy=0.685]\u001b[A\n",
      " 47%|███████████████████████▋                          | 428/904 [00:16<00:05, 87.1batch/s, loss=0.492, accuracy=0.688]\u001b[A\n",
      " 48%|████████████████████████▋                          | 437/904 [00:16<00:05, 86.8batch/s, loss=0.492, accuracy=0.69]\u001b[A\n",
      " 49%|████████████████████████▋                         | 446/904 [00:16<00:05, 87.3batch/s, loss=0.487, accuracy=0.692]\u001b[A\n",
      " 50%|█████████████████████████▏                        | 455/904 [00:16<00:05, 87.3batch/s, loss=0.485, accuracy=0.695]\u001b[A\n",
      " 51%|█████████████████████████▋                        | 464/904 [00:17<00:05, 85.9batch/s, loss=0.482, accuracy=0.697]\u001b[A\n",
      " 52%|██████████████████████████▏                       | 473/904 [00:17<00:05, 84.1batch/s, loss=0.479, accuracy=0.699]\u001b[A\n",
      " 53%|██████████████████████████▋                       | 482/904 [00:17<00:05, 84.2batch/s, loss=0.477, accuracy=0.701]\u001b[A\n",
      " 54%|███████████████████████████▏                      | 491/904 [00:17<00:04, 83.4batch/s, loss=0.475, accuracy=0.702]\u001b[A\n",
      " 55%|███████████████████████████▋                      | 501/904 [00:17<00:04, 85.4batch/s, loss=0.472, accuracy=0.704]\u001b[A\n",
      " 56%|████████████████████████████▏                     | 510/904 [00:17<00:04, 85.9batch/s, loss=0.469, accuracy=0.707]\u001b[A\n",
      " 58%|████████████████████████████▊                     | 520/904 [00:17<00:04, 88.7batch/s, loss=0.465, accuracy=0.709]\u001b[A\n",
      " 59%|█████████████████████████████▎                    | 529/904 [00:17<00:04, 88.3batch/s, loss=0.461, accuracy=0.712]\u001b[A\n",
      " 60%|█████████████████████████████▊                    | 538/904 [00:17<00:04, 88.5batch/s, loss=0.456, accuracy=0.715]\u001b[A\n",
      " 61%|██████████████████████████████▎                   | 547/904 [00:17<00:04, 88.6batch/s, loss=0.452, accuracy=0.717]\u001b[A\n",
      " 62%|██████████████████████████████▊                   | 557/904 [00:18<00:03, 89.6batch/s, loss=0.447, accuracy=0.721]\u001b[A\n",
      " 63%|███████████████████████████████▎                  | 566/904 [00:18<00:03, 88.2batch/s, loss=0.443, accuracy=0.723]\u001b[A\n",
      " 64%|███████████████████████████████▊                  | 576/904 [00:18<00:03, 88.4batch/s, loss=0.441, accuracy=0.726]\u001b[A\n",
      " 65%|████████████████████████████████▎                 | 585/904 [00:18<00:03, 88.3batch/s, loss=0.437, accuracy=0.729]\u001b[A\n",
      " 66%|█████████████████████████████████▌                 | 595/904 [00:18<00:03, 89.4batch/s, loss=0.436, accuracy=0.73]\u001b[A\n",
      " 67%|█████████████████████████████████▍                | 604/904 [00:18<00:03, 88.1batch/s, loss=0.433, accuracy=0.733]\u001b[A\n",
      " 68%|█████████████████████████████████▉                | 613/904 [00:18<00:03, 86.2batch/s, loss=0.431, accuracy=0.735]\u001b[A\n",
      " 69%|██████████████████████████████████▍               | 623/904 [00:18<00:03, 88.9batch/s, loss=0.427, accuracy=0.737]\u001b[A\n",
      " 70%|██████████████████████████████████▉               | 632/904 [00:18<00:03, 89.1batch/s, loss=0.427, accuracy=0.738]\u001b[A\n",
      " 71%|███████████████████████████████████▍              | 641/904 [00:19<00:03, 86.8batch/s, loss=0.425, accuracy=0.739]\u001b[A\n",
      " 72%|████████████████████████████████████▋              | 650/904 [00:19<00:03, 83.5batch/s, loss=0.423, accuracy=0.74]\u001b[A\n",
      " 73%|█████████████████████████████████████▏             | 659/904 [00:19<00:03, 81.2batch/s, loss=0.42, accuracy=0.742]\u001b[A\n",
      " 74%|████████████████████████████████████▉             | 668/904 [00:19<00:02, 81.4batch/s, loss=0.417, accuracy=0.743]\u001b[A\n",
      " 75%|█████████████████████████████████████▍            | 677/904 [00:19<00:02, 83.5batch/s, loss=0.414, accuracy=0.745]\u001b[A\n",
      " 76%|█████████████████████████████████████▉            | 686/904 [00:19<00:02, 85.1batch/s, loss=0.412, accuracy=0.747]\u001b[A\n",
      " 77%|██████████████████████████████████████▍           | 696/904 [00:19<00:02, 86.5batch/s, loss=0.409, accuracy=0.748]\u001b[A\n",
      " 78%|██████████████████████████████████████▉           | 705/904 [00:19<00:02, 86.8batch/s, loss=0.407, accuracy=0.749]\u001b[A\n",
      " 79%|███████████████████████████████████████▌          | 715/904 [00:19<00:02, 87.5batch/s, loss=0.404, accuracy=0.751]\u001b[A\n",
      " 80%|████████████████████████████████████████          | 724/904 [00:20<00:02, 87.5batch/s, loss=0.401, accuracy=0.753]\u001b[A\n",
      " 81%|████████████████████████████████████████▌         | 733/904 [00:20<00:01, 86.9batch/s, loss=0.398, accuracy=0.754]\u001b[A\n",
      " 82%|█████████████████████████████████████████         | 743/904 [00:20<00:01, 88.3batch/s, loss=0.395, accuracy=0.756]\u001b[A\n",
      " 83%|█████████████████████████████████████████▋        | 753/904 [00:20<00:01, 88.6batch/s, loss=0.392, accuracy=0.758]\u001b[A\n",
      " 84%|██████████████████████████████████████████▉        | 762/904 [00:20<00:01, 85.8batch/s, loss=0.389, accuracy=0.76]\u001b[A\n",
      " 85%|██████████████████████████████████████████▋       | 771/904 [00:20<00:01, 84.6batch/s, loss=0.387, accuracy=0.761]\u001b[A\n",
      " 86%|███████████████████████████████████████████▏      | 780/904 [00:20<00:01, 85.1batch/s, loss=0.384, accuracy=0.763]\u001b[A\n",
      " 87%|███████████████████████████████████████████▋      | 789/904 [00:20<00:01, 86.0batch/s, loss=0.384, accuracy=0.764]\u001b[A\n",
      " 88%|████████████████████████████████████████████▏     | 798/904 [00:20<00:01, 85.6batch/s, loss=0.384, accuracy=0.764]\u001b[A\n",
      " 89%|████████████████████████████████████████████▋     | 807/904 [00:20<00:01, 86.1batch/s, loss=0.383, accuracy=0.764]\u001b[A\n",
      " 90%|█████████████████████████████████████████████▏    | 816/904 [00:21<00:01, 85.5batch/s, loss=0.381, accuracy=0.766]\u001b[A\n",
      " 91%|█████████████████████████████████████████████▋    | 825/904 [00:21<00:00, 84.6batch/s, loss=0.379, accuracy=0.766]\u001b[A\n",
      " 92%|██████████████████████████████████████████████▏   | 834/904 [00:21<00:00, 85.9batch/s, loss=0.379, accuracy=0.768]\u001b[A\n",
      " 93%|██████████████████████████████████████████████▋   | 843/904 [00:21<00:00, 86.0batch/s, loss=0.377, accuracy=0.769]\u001b[A\n",
      " 94%|████████████████████████████████████████████████   | 852/904 [00:21<00:00, 86.3batch/s, loss=0.375, accuracy=0.77]\u001b[A\n",
      " 95%|███████████████████████████████████████████████▌  | 861/904 [00:21<00:00, 86.9batch/s, loss=0.374, accuracy=0.771]\u001b[A\n",
      " 96%|████████████████████████████████████████████████▏ | 871/904 [00:21<00:00, 88.0batch/s, loss=0.372, accuracy=0.772]\u001b[A\n",
      " 97%|████████████████████████████████████████████████▋ | 880/904 [00:21<00:00, 85.9batch/s, loss=0.371, accuracy=0.773]\u001b[A\n",
      " 98%|█████████████████████████████████████████████████▏| 889/904 [00:21<00:00, 84.6batch/s, loss=0.369, accuracy=0.774]\u001b[A\n",
      "  3%|▏     | 1/30 [00:25<12:14, 25.33s/epoch, loss=0.366, accuracy=0.776, val_loss=0.959, val_accuracy=0.786, lr=0.001]\u001b[A\n",
      "  0%|                                                        | 0.00/904 [00:00<?, ?batch/s, loss=0.366, accuracy=0.776]\u001b[A\n",
      "  1%|▌                                                 | 10.0/904 [00:00<00:09, 91.7batch/s, loss=0.26, accuracy=0.869]\u001b[A\n",
      "  2%|█                                                | 20.0/904 [00:00<00:10, 87.8batch/s, loss=0.216, accuracy=0.878]\u001b[A\n",
      "  3%|█▌                                               | 29.0/904 [00:00<00:10, 82.6batch/s, loss=0.204, accuracy=0.886]\u001b[A\n",
      "  4%|██                                               | 38.0/904 [00:00<00:10, 84.6batch/s, loss=0.192, accuracy=0.884]\u001b[A\n",
      "  5%|██▌                                              | 47.0/904 [00:00<00:09, 86.1batch/s, loss=0.186, accuracy=0.886]\u001b[A\n",
      "  6%|███                                              | 56.0/904 [00:00<00:10, 84.4batch/s, loss=0.184, accuracy=0.888]\u001b[A\n",
      "  7%|███▌                                             | 65.0/904 [00:00<00:09, 84.1batch/s, loss=0.177, accuracy=0.889]\u001b[A\n",
      "  8%|████                                             | 74.0/904 [00:00<00:09, 85.4batch/s, loss=0.178, accuracy=0.892]\u001b[A\n",
      "  9%|████▍                                            | 83.0/904 [00:00<00:09, 85.4batch/s, loss=0.171, accuracy=0.895]\u001b[A\n",
      " 10%|████▉                                            | 92.0/904 [00:01<00:09, 85.8batch/s, loss=0.167, accuracy=0.897]\u001b[A\n",
      " 11%|█████▌                                            | 101/904 [00:01<00:09, 86.4batch/s, loss=0.166, accuracy=0.897]\u001b[A\n",
      " 12%|██████                                            | 110/904 [00:01<00:09, 84.8batch/s, loss=0.173, accuracy=0.898]\u001b[A\n",
      " 13%|██████▌                                           | 119/904 [00:01<00:09, 85.6batch/s, loss=0.175, accuracy=0.899]\u001b[A\n",
      " 14%|███████                                           | 128/904 [00:01<00:09, 85.3batch/s, loss=0.188, accuracy=0.895]\u001b[A\n",
      " 15%|███████▌                                          | 137/904 [00:01<00:09, 83.2batch/s, loss=0.199, accuracy=0.891]\u001b[A\n",
      " 16%|████████                                          | 146/904 [00:01<00:08, 84.7batch/s, loss=0.218, accuracy=0.878]\u001b[A\n",
      " 17%|████████▌                                         | 155/904 [00:01<00:08, 85.2batch/s, loss=0.232, accuracy=0.869]\u001b[A\n",
      " 18%|█████████                                         | 164/904 [00:01<00:08, 85.6batch/s, loss=0.239, accuracy=0.868]\u001b[A\n",
      " 19%|█████████▌                                        | 173/904 [00:02<00:08, 86.1batch/s, loss=0.241, accuracy=0.868]\u001b[A\n",
      " 20%|██████████                                        | 182/904 [00:02<00:08, 86.2batch/s, loss=0.242, accuracy=0.869]\u001b[A\n",
      " 21%|██████████▌                                       | 191/904 [00:02<00:08, 86.0batch/s, loss=0.241, accuracy=0.868]\u001b[A\n",
      " 22%|███████████                                       | 200/904 [00:02<00:08, 86.0batch/s, loss=0.241, accuracy=0.868]\u001b[A\n",
      " 23%|███████████▌                                      | 209/904 [00:02<00:08, 81.7batch/s, loss=0.238, accuracy=0.867]\u001b[A\n",
      " 24%|████████████                                      | 218/904 [00:02<00:08, 82.2batch/s, loss=0.236, accuracy=0.869]\u001b[A\n",
      " 25%|████████████▊                                      | 227/904 [00:02<00:08, 83.8batch/s, loss=0.235, accuracy=0.87]\u001b[A\n",
      " 26%|█████████████                                     | 236/904 [00:02<00:08, 83.0batch/s, loss=0.232, accuracy=0.872]\u001b[A\n",
      " 27%|█████████████▌                                    | 245/904 [00:02<00:07, 84.1batch/s, loss=0.228, accuracy=0.873]\u001b[A\n",
      " 28%|██████████████                                    | 254/904 [00:02<00:07, 84.3batch/s, loss=0.226, accuracy=0.873]\u001b[A\n",
      " 29%|██████████████▌                                   | 263/904 [00:03<00:07, 83.3batch/s, loss=0.222, accuracy=0.875]\u001b[A\n",
      " 30%|███████████████                                   | 272/904 [00:03<00:07, 84.7batch/s, loss=0.224, accuracy=0.875]\u001b[A\n",
      " 31%|███████████████▌                                  | 281/904 [00:03<00:07, 84.1batch/s, loss=0.224, accuracy=0.875]\u001b[A\n",
      " 32%|████████████████                                  | 291/904 [00:03<00:07, 86.6batch/s, loss=0.221, accuracy=0.875]\u001b[A\n",
      " 33%|████████████████▉                                  | 300/904 [00:03<00:07, 84.5batch/s, loss=0.22, accuracy=0.875]\u001b[A\n",
      " 34%|█████████████████                                 | 309/904 [00:03<00:07, 84.4batch/s, loss=0.219, accuracy=0.876]\u001b[A\n",
      " 35%|█████████████████▌                                | 318/904 [00:03<00:06, 85.6batch/s, loss=0.215, accuracy=0.878]\u001b[A\n",
      " 36%|██████████████████                                | 327/904 [00:03<00:06, 85.1batch/s, loss=0.213, accuracy=0.879]\u001b[A\n",
      " 37%|███████████████████▎                                | 336/904 [00:03<00:06, 85.6batch/s, loss=0.21, accuracy=0.88]\u001b[A\n",
      " 38%|███████████████████                               | 345/904 [00:04<00:06, 84.3batch/s, loss=0.208, accuracy=0.882]\u001b[A\n",
      " 39%|███████████████████▌                              | 354/904 [00:04<00:06, 84.1batch/s, loss=0.205, accuracy=0.883]\u001b[A\n",
      " 40%|████████████████████                              | 363/904 [00:04<00:06, 84.7batch/s, loss=0.203, accuracy=0.884]\u001b[A\n",
      " 41%|████████████████████▌                             | 372/904 [00:04<00:06, 83.5batch/s, loss=0.201, accuracy=0.885]\u001b[A\n",
      " 42%|█████████████████████                             | 381/904 [00:04<00:06, 84.2batch/s, loss=0.199, accuracy=0.886]\u001b[A\n",
      " 43%|██████████████████████▍                             | 390/904 [00:04<00:06, 83.1batch/s, loss=0.2, accuracy=0.886]\u001b[A\n",
      " 44%|██████████████████████                            | 399/904 [00:04<00:05, 84.2batch/s, loss=0.199, accuracy=0.886]\u001b[A\n",
      " 45%|██████████████████████▌                           | 408/904 [00:04<00:05, 82.7batch/s, loss=0.197, accuracy=0.887]\u001b[A\n",
      " 46%|███████████████████████                           | 417/904 [00:04<00:05, 83.5batch/s, loss=0.196, accuracy=0.887]\u001b[A\n",
      " 47%|███████████████████████▌                          | 426/904 [00:05<00:05, 82.9batch/s, loss=0.195, accuracy=0.888]\u001b[A\n",
      " 48%|████████████████████████                          | 435/904 [00:05<00:05, 82.1batch/s, loss=0.193, accuracy=0.889]\u001b[A\n",
      " 49%|████████████████████████▌                         | 444/904 [00:05<00:05, 83.3batch/s, loss=0.191, accuracy=0.889]\u001b[A\n",
      " 50%|██████████████████████████                          | 453/904 [00:05<00:05, 82.1batch/s, loss=0.19, accuracy=0.89]\u001b[A\n",
      " 51%|██████████████████████████                         | 462/904 [00:05<00:05, 81.5batch/s, loss=0.188, accuracy=0.89]\u001b[A\n",
      " 52%|██████████████████████████                        | 471/904 [00:05<00:05, 80.9batch/s, loss=0.187, accuracy=0.891]\u001b[A\n",
      " 53%|██████████████████████████▌                       | 480/904 [00:05<00:05, 81.7batch/s, loss=0.186, accuracy=0.892]\u001b[A\n",
      " 54%|███████████████████████████                       | 489/904 [00:05<00:05, 82.1batch/s, loss=0.185, accuracy=0.893]\u001b[A\n",
      " 55%|███████████████████████████▌                      | 498/904 [00:05<00:04, 82.6batch/s, loss=0.185, accuracy=0.893]\u001b[A\n",
      " 56%|████████████████████████████                      | 507/904 [00:06<00:04, 83.4batch/s, loss=0.184, accuracy=0.893]\u001b[A\n",
      " 57%|████████████████████████████▌                     | 516/904 [00:06<00:04, 83.3batch/s, loss=0.182, accuracy=0.894]\u001b[A\n",
      " 58%|█████████████████████████████                     | 525/904 [00:06<00:04, 84.0batch/s, loss=0.181, accuracy=0.894]\u001b[A\n",
      " 59%|█████████████████████████████▌                    | 534/904 [00:06<00:04, 83.6batch/s, loss=0.181, accuracy=0.895]\u001b[A\n",
      " 60%|██████████████████████████████▋                    | 543/904 [00:06<00:04, 82.6batch/s, loss=0.18, accuracy=0.896]\u001b[A\n",
      " 61%|███████████████████████████████▏                   | 552/904 [00:06<00:04, 83.9batch/s, loss=0.18, accuracy=0.896]\u001b[A\n",
      " 62%|███████████████████████████████                   | 561/904 [00:06<00:04, 83.7batch/s, loss=0.178, accuracy=0.896]\u001b[A\n",
      " 63%|███████████████████████████████▌                  | 570/904 [00:06<00:04, 82.6batch/s, loss=0.177, accuracy=0.896]\u001b[A\n",
      " 64%|████████████████████████████████                  | 579/904 [00:06<00:03, 84.4batch/s, loss=0.177, accuracy=0.896]\u001b[A\n",
      " 65%|████████████████████████████████▌                 | 588/904 [00:06<00:03, 84.8batch/s, loss=0.175, accuracy=0.897]\u001b[A\n",
      " 66%|█████████████████████████████████                 | 597/904 [00:07<00:03, 85.4batch/s, loss=0.174, accuracy=0.897]\u001b[A\n",
      " 67%|█████████████████████████████████▌                | 606/904 [00:07<00:03, 86.2batch/s, loss=0.174, accuracy=0.898]\u001b[A\n",
      " 68%|██████████████████████████████████                | 615/904 [00:07<00:03, 84.0batch/s, loss=0.174, accuracy=0.898]\u001b[A\n",
      " 69%|██████████████████████████████████▌               | 624/904 [00:07<00:03, 85.7batch/s, loss=0.174, accuracy=0.899]\u001b[A\n",
      " 70%|████████████████████████████████████▍               | 633/904 [00:07<00:03, 86.9batch/s, loss=0.173, accuracy=0.9]\u001b[A\n",
      " 71%|████████████████████████████████████▉               | 642/904 [00:07<00:03, 84.7batch/s, loss=0.172, accuracy=0.9]\u001b[A\n",
      " 72%|█████████████████████████████████████▍              | 651/904 [00:07<00:02, 85.0batch/s, loss=0.171, accuracy=0.9]\u001b[A\n",
      " 73%|█████████████████████████████████████▉              | 660/904 [00:07<00:02, 84.8batch/s, loss=0.171, accuracy=0.9]\u001b[A\n",
      " 74%|█████████████████████████████████████             | 669/904 [00:07<00:02, 84.4batch/s, loss=0.171, accuracy=0.901]\u001b[A\n",
      " 75%|██████████████████████████████████████▎            | 678/904 [00:08<00:02, 85.8batch/s, loss=0.17, accuracy=0.901]\u001b[A\n",
      " 76%|█████████████████████████████████████▉            | 687/904 [00:08<00:02, 84.8batch/s, loss=0.169, accuracy=0.902]\u001b[A\n",
      " 77%|██████████████████████████████████████▍           | 696/904 [00:08<00:02, 84.3batch/s, loss=0.168, accuracy=0.902]\u001b[A\n",
      " 78%|██████████████████████████████████████▉           | 705/904 [00:08<00:02, 81.3batch/s, loss=0.167, accuracy=0.902]\u001b[A\n",
      " 79%|███████████████████████████████████████▍          | 714/904 [00:08<00:02, 82.0batch/s, loss=0.167, accuracy=0.902]\u001b[A\n",
      " 80%|███████████████████████████████████████▉          | 723/904 [00:08<00:02, 82.5batch/s, loss=0.166, accuracy=0.903]\u001b[A\n",
      " 81%|████████████████████████████████████████▍         | 732/904 [00:08<00:02, 82.9batch/s, loss=0.166, accuracy=0.903]\u001b[A\n",
      " 82%|████████████████████████████████████████▉         | 741/904 [00:08<00:01, 84.1batch/s, loss=0.165, accuracy=0.903]\u001b[A\n",
      " 83%|█████████████████████████████████████████▍        | 750/904 [00:08<00:01, 84.4batch/s, loss=0.165, accuracy=0.903]\u001b[A\n",
      " 84%|█████████████████████████████████████████▉        | 759/904 [00:09<00:01, 83.9batch/s, loss=0.165, accuracy=0.903]\u001b[A\n",
      " 85%|██████████████████████████████████████████▍       | 768/904 [00:09<00:01, 83.6batch/s, loss=0.164, accuracy=0.904]\u001b[A\n",
      " 86%|██████████████████████████████████████████▉       | 777/904 [00:09<00:01, 83.3batch/s, loss=0.164, accuracy=0.904]\u001b[A\n",
      " 87%|███████████████████████████████████████████▍      | 786/904 [00:09<00:01, 81.7batch/s, loss=0.164, accuracy=0.904]\u001b[A\n",
      " 88%|███████████████████████████████████████████▉      | 795/904 [00:09<00:01, 82.7batch/s, loss=0.163, accuracy=0.904]\u001b[A\n",
      " 89%|████████████████████████████████████████████▍     | 804/904 [00:09<00:01, 83.6batch/s, loss=0.163, accuracy=0.904]\u001b[A\n",
      " 90%|████████████████████████████████████████████▉     | 813/904 [00:09<00:01, 83.8batch/s, loss=0.163, accuracy=0.905]\u001b[A\n",
      " 91%|█████████████████████████████████████████████▌    | 823/904 [00:09<00:00, 85.6batch/s, loss=0.163, accuracy=0.905]\u001b[A\n",
      " 92%|██████████████████████████████████████████████    | 832/904 [00:09<00:00, 83.7batch/s, loss=0.163, accuracy=0.904]\u001b[A\n",
      " 93%|██████████████████████████████████████████████▌   | 841/904 [00:09<00:00, 85.1batch/s, loss=0.162, accuracy=0.904]\u001b[A\n",
      " 94%|███████████████████████████████████████████████   | 850/904 [00:10<00:00, 84.7batch/s, loss=0.163, accuracy=0.904]\u001b[A\n",
      " 95%|███████████████████████████████████████████████▌  | 859/904 [00:10<00:00, 85.0batch/s, loss=0.163, accuracy=0.904]\u001b[A\n",
      " 96%|████████████████████████████████████████████████  | 868/904 [00:10<00:00, 83.2batch/s, loss=0.163, accuracy=0.904]\u001b[A\n",
      " 97%|████████████████████████████████████████████████▌ | 877/904 [00:10<00:00, 84.4batch/s, loss=0.162, accuracy=0.904]\u001b[A\n",
      " 98%|█████████████████████████████████████████████████ | 886/904 [00:10<00:00, 84.5batch/s, loss=0.162, accuracy=0.904]\u001b[A\n",
      " 99%|█████████████████████████████████████████████████▌| 895/904 [00:10<00:00, 84.2batch/s, loss=0.165, accuracy=0.904]\u001b[A\n",
      "  7%|▍     | 2/30 [00:38<08:22, 17.94s/epoch, loss=0.165, accuracy=0.904, val_loss=0.968, val_accuracy=0.798, lr=0.001]\u001b[A\n",
      "  0%|                                                        | 0.00/904 [00:00<?, ?batch/s, loss=0.165, accuracy=0.904]\u001b[A\n",
      "  1%|▍                                                | 9.00/904 [00:00<00:11, 74.7batch/s, loss=0.124, accuracy=0.948]\u001b[A\n",
      "  2%|▉                                                | 18.0/904 [00:00<00:11, 80.5batch/s, loss=0.166, accuracy=0.931]\u001b[A\n",
      "  3%|█▍                                               | 27.0/904 [00:00<00:10, 83.3batch/s, loss=0.147, accuracy=0.931]\u001b[A\n",
      "  4%|█▉                                               | 36.0/904 [00:00<00:10, 83.6batch/s, loss=0.143, accuracy=0.919]\u001b[A\n",
      "  5%|██▍                                              | 45.0/904 [00:00<00:10, 82.4batch/s, loss=0.146, accuracy=0.919]\u001b[A\n",
      "  6%|███                                                | 54.0/904 [00:00<00:10, 82.9batch/s, loss=0.14, accuracy=0.92]\u001b[A\n",
      "  7%|███▍                                             | 63.0/904 [00:00<00:10, 82.5batch/s, loss=0.136, accuracy=0.919]\u001b[A\n",
      "  8%|███▉                                             | 72.0/904 [00:00<00:10, 82.3batch/s, loss=0.148, accuracy=0.916]\u001b[A\n",
      "  9%|████▍                                            | 81.0/904 [00:00<00:10, 82.2batch/s, loss=0.148, accuracy=0.914]\u001b[A\n",
      " 10%|████▉                                            | 90.0/904 [00:01<00:09, 83.2batch/s, loss=0.144, accuracy=0.915]\u001b[A\n",
      " 11%|█████▎                                           | 99.0/904 [00:01<00:09, 84.0batch/s, loss=0.156, accuracy=0.913]\u001b[A\n",
      " 12%|█████▉                                            | 108/904 [00:01<00:09, 84.8batch/s, loss=0.155, accuracy=0.911]\u001b[A\n",
      " 13%|██████▍                                           | 117/904 [00:01<00:09, 84.8batch/s, loss=0.156, accuracy=0.911]\u001b[A\n",
      " 14%|███████                                            | 126/904 [00:01<00:09, 84.8batch/s, loss=0.159, accuracy=0.91]\u001b[A\n",
      " 15%|███████▌                                           | 135/904 [00:01<00:09, 83.2batch/s, loss=0.159, accuracy=0.91]\u001b[A\n",
      " 16%|████████▎                                           | 144/904 [00:01<00:09, 83.4batch/s, loss=0.16, accuracy=0.91]\u001b[A\n",
      " 17%|████████▍                                         | 153/904 [00:01<00:08, 83.5batch/s, loss=0.158, accuracy=0.911]\u001b[A\n",
      " 18%|█████████▎                                          | 162/904 [00:01<00:08, 83.7batch/s, loss=0.16, accuracy=0.91]\u001b[A\n",
      " 19%|█████████▍                                        | 171/904 [00:02<00:08, 82.8batch/s, loss=0.158, accuracy=0.911]\u001b[A\n",
      " 20%|█████████▉                                        | 180/904 [00:02<00:08, 83.7batch/s, loss=0.161, accuracy=0.911]\u001b[A\n",
      " 21%|██████████▍                                       | 189/904 [00:02<00:08, 82.2batch/s, loss=0.158, accuracy=0.912]\u001b[A\n",
      " 22%|██████████▉                                       | 198/904 [00:02<00:08, 83.6batch/s, loss=0.156, accuracy=0.913]\u001b[A\n",
      " 23%|███████████▍                                      | 207/904 [00:02<00:08, 82.2batch/s, loss=0.153, accuracy=0.915]\u001b[A\n",
      " 24%|███████████▉                                      | 216/904 [00:02<00:08, 84.3batch/s, loss=0.151, accuracy=0.915]\u001b[A\n",
      " 25%|████████████▍                                     | 225/904 [00:02<00:08, 84.0batch/s, loss=0.148, accuracy=0.917]\u001b[A\n",
      " 26%|████████████▉                                     | 234/904 [00:02<00:08, 83.7batch/s, loss=0.145, accuracy=0.918]\u001b[A\n",
      " 27%|█████████████▍                                    | 243/904 [00:02<00:07, 85.4batch/s, loss=0.144, accuracy=0.918]\u001b[A\n",
      " 28%|█████████████▉                                    | 252/904 [00:03<00:07, 86.4batch/s, loss=0.142, accuracy=0.919]\u001b[A\n",
      " 29%|███████████████                                     | 261/904 [00:03<00:07, 85.4batch/s, loss=0.14, accuracy=0.92]\u001b[A\n",
      " 30%|██████████████▉                                   | 271/904 [00:03<00:07, 86.5batch/s, loss=0.139, accuracy=0.921]\u001b[A\n",
      " 31%|███████████████▍                                  | 280/904 [00:03<00:07, 85.7batch/s, loss=0.138, accuracy=0.921]\u001b[A\n",
      " 32%|███████████████▉                                  | 289/904 [00:03<00:07, 82.1batch/s, loss=0.136, accuracy=0.922]\u001b[A\n",
      " 33%|████████████████▍                                 | 298/904 [00:03<00:07, 83.9batch/s, loss=0.135, accuracy=0.922]\u001b[A\n",
      " 34%|████████████████▉                                 | 307/904 [00:03<00:07, 85.1batch/s, loss=0.136, accuracy=0.922]\u001b[A\n",
      " 35%|█████████████████▍                                | 316/904 [00:03<00:06, 84.2batch/s, loss=0.136, accuracy=0.921]\u001b[A\n",
      " 36%|█████████████████▉                                | 325/904 [00:03<00:06, 85.3batch/s, loss=0.139, accuracy=0.921]\u001b[A\n",
      " 37%|██████████████████▍                               | 334/904 [00:03<00:06, 84.7batch/s, loss=0.141, accuracy=0.921]\u001b[A\n",
      " 38%|███████████████████▎                               | 343/904 [00:04<00:06, 85.4batch/s, loss=0.14, accuracy=0.921]\u001b[A\n",
      " 39%|███████████████████▍                              | 352/904 [00:04<00:06, 83.5batch/s, loss=0.139, accuracy=0.921]\u001b[A\n",
      " 40%|███████████████████▉                              | 361/904 [00:04<00:06, 85.0batch/s, loss=0.139, accuracy=0.921]\u001b[A\n",
      " 41%|████████████████████▍                             | 370/904 [00:04<00:06, 85.6batch/s, loss=0.137, accuracy=0.921]\u001b[A\n",
      " 42%|████████████████████▉                             | 379/904 [00:04<00:06, 85.0batch/s, loss=0.136, accuracy=0.922]\u001b[A\n",
      " 43%|█████████████████████▍                            | 388/904 [00:04<00:06, 83.9batch/s, loss=0.135, accuracy=0.922]\u001b[A\n",
      " 44%|█████████████████████▉                            | 397/904 [00:04<00:05, 84.5batch/s, loss=0.134, accuracy=0.923]\u001b[A\n",
      " 45%|██████████████████████▍                           | 406/904 [00:04<00:05, 83.7batch/s, loss=0.133, accuracy=0.923]\u001b[A\n",
      " 46%|██████████████████████▉                           | 415/904 [00:04<00:05, 83.4batch/s, loss=0.133, accuracy=0.923]\u001b[A\n",
      " 47%|███████████████████████▍                          | 424/904 [00:05<00:05, 84.2batch/s, loss=0.132, accuracy=0.924]\u001b[A\n",
      " 48%|███████████████████████▉                          | 433/904 [00:05<00:05, 83.4batch/s, loss=0.131, accuracy=0.924]\u001b[A\n",
      " 49%|████████████████████████▉                          | 442/904 [00:05<00:05, 84.6batch/s, loss=0.13, accuracy=0.925]\u001b[A\n",
      " 50%|█████████████████████████                         | 452/904 [00:05<00:05, 86.0batch/s, loss=0.129, accuracy=0.925]\u001b[A\n",
      " 51%|█████████████████████████▍                        | 461/904 [00:05<00:05, 85.6batch/s, loss=0.129, accuracy=0.925]\u001b[A\n",
      " 52%|█████████████████████████▉                        | 470/904 [00:05<00:05, 85.8batch/s, loss=0.129, accuracy=0.925]\u001b[A\n",
      " 53%|██████████████████████████▍                       | 479/904 [00:05<00:05, 81.9batch/s, loss=0.128, accuracy=0.925]\u001b[A\n",
      " 54%|███████████████████████████                       | 489/904 [00:05<00:04, 85.1batch/s, loss=0.127, accuracy=0.926]\u001b[A\n",
      " 55%|███████████████████████████▌                      | 498/904 [00:05<00:04, 85.9batch/s, loss=0.127, accuracy=0.925]\u001b[A\n",
      " 56%|████████████████████████████                      | 507/904 [00:06<00:04, 84.7batch/s, loss=0.127, accuracy=0.925]\u001b[A\n",
      " 57%|████████████████████████████▌                     | 516/904 [00:06<00:04, 85.0batch/s, loss=0.126, accuracy=0.925]\u001b[A\n",
      " 58%|█████████████████████████████                     | 525/904 [00:06<00:04, 85.3batch/s, loss=0.126, accuracy=0.925]\u001b[A\n",
      " 59%|█████████████████████████████▌                    | 534/904 [00:06<00:04, 84.7batch/s, loss=0.126, accuracy=0.925]\u001b[A\n",
      " 60%|██████████████████████████████                    | 543/904 [00:06<00:04, 85.4batch/s, loss=0.127, accuracy=0.925]\u001b[A\n",
      " 61%|██████████████████████████████▌                   | 552/904 [00:06<00:04, 84.8batch/s, loss=0.127, accuracy=0.925]\u001b[A\n",
      " 62%|███████████████████████████████                   | 561/904 [00:06<00:04, 84.2batch/s, loss=0.127, accuracy=0.924]\u001b[A\n",
      " 63%|███████████████████████████████▌                  | 570/904 [00:06<00:04, 81.9batch/s, loss=0.127, accuracy=0.924]\u001b[A\n",
      " 64%|████████████████████████████████                  | 579/904 [00:06<00:03, 82.3batch/s, loss=0.128, accuracy=0.924]\u001b[A\n",
      " 65%|████████████████████████████████▌                 | 588/904 [00:06<00:03, 83.8batch/s, loss=0.127, accuracy=0.924]\u001b[A\n",
      " 66%|█████████████████████████████████                 | 597/904 [00:07<00:03, 84.2batch/s, loss=0.127, accuracy=0.924]\u001b[A\n",
      " 67%|█████████████████████████████████▌                | 606/904 [00:07<00:03, 85.4batch/s, loss=0.126, accuracy=0.924]\u001b[A\n",
      " 68%|██████████████████████████████████                | 615/904 [00:07<00:03, 84.5batch/s, loss=0.126, accuracy=0.924]\u001b[A\n",
      " 69%|██████████████████████████████████▌               | 625/904 [00:07<00:03, 87.2batch/s, loss=0.126, accuracy=0.924]\u001b[A\n",
      " 70%|███████████████████████████████████               | 634/904 [00:07<00:03, 87.3batch/s, loss=0.126, accuracy=0.924]\u001b[A\n",
      " 71%|███████████████████████████████████▌              | 643/904 [00:07<00:03, 86.5batch/s, loss=0.126, accuracy=0.924]\u001b[A\n",
      " 72%|████████████████████████████████████              | 652/904 [00:07<00:02, 86.5batch/s, loss=0.126, accuracy=0.924]\u001b[A\n",
      " 73%|████████████████████████████████████▌             | 661/904 [00:07<00:02, 84.4batch/s, loss=0.125, accuracy=0.924]\u001b[A\n",
      " 74%|█████████████████████████████████████             | 670/904 [00:07<00:02, 84.7batch/s, loss=0.125, accuracy=0.924]\u001b[A\n",
      " 75%|█████████████████████████████████████▌            | 679/904 [00:08<00:02, 85.4batch/s, loss=0.124, accuracy=0.924]\u001b[A\n",
      " 76%|██████████████████████████████████████            | 688/904 [00:08<00:02, 85.5batch/s, loss=0.124, accuracy=0.924]\u001b[A\n",
      " 77%|██████████████████████████████████████▌           | 697/904 [00:08<00:02, 85.2batch/s, loss=0.123, accuracy=0.924]\u001b[A\n",
      " 78%|███████████████████████████████████████           | 706/904 [00:08<00:02, 86.4batch/s, loss=0.123, accuracy=0.924]\u001b[A\n",
      " 79%|███████████████████████████████████████▌          | 715/904 [00:08<00:02, 86.6batch/s, loss=0.123, accuracy=0.924]\u001b[A\n",
      " 80%|████████████████████████████████████████          | 724/904 [00:08<00:02, 86.6batch/s, loss=0.123, accuracy=0.924]\u001b[A\n",
      " 81%|████████████████████████████████████████▌         | 733/904 [00:08<00:02, 85.5batch/s, loss=0.122, accuracy=0.924]\u001b[A\n",
      " 82%|█████████████████████████████████████████         | 742/904 [00:08<00:01, 85.7batch/s, loss=0.122, accuracy=0.923]\u001b[A\n",
      " 83%|█████████████████████████████████████████▌        | 751/904 [00:08<00:01, 86.8batch/s, loss=0.122, accuracy=0.923]\u001b[A\n",
      " 84%|██████████████████████████████████████████        | 760/904 [00:08<00:01, 86.0batch/s, loss=0.121, accuracy=0.924]\u001b[A\n",
      " 85%|██████████████████████████████████████████▌       | 769/904 [00:09<00:01, 85.0batch/s, loss=0.121, accuracy=0.924]\u001b[A\n",
      " 86%|███████████████████████████████████████████       | 778/904 [00:09<00:01, 86.0batch/s, loss=0.121, accuracy=0.924]\u001b[A\n",
      " 87%|████████████████████████████████████████████▍      | 787/904 [00:09<00:01, 84.1batch/s, loss=0.12, accuracy=0.924]\u001b[A\n",
      " 88%|████████████████████████████████████████████▉      | 796/904 [00:09<00:01, 85.4batch/s, loss=0.12, accuracy=0.924]\u001b[A\n",
      " 89%|█████████████████████████████████████████████▍     | 805/904 [00:09<00:01, 85.0batch/s, loss=0.12, accuracy=0.924]\u001b[A\n",
      " 90%|█████████████████████████████████████████████▉     | 814/904 [00:09<00:01, 84.7batch/s, loss=0.12, accuracy=0.924]\u001b[A\n",
      " 91%|██████████████████████████████████████████████▍    | 823/904 [00:09<00:00, 83.0batch/s, loss=0.12, accuracy=0.924]\u001b[A\n",
      " 92%|██████████████████████████████████████████████    | 832/904 [00:09<00:00, 79.7batch/s, loss=0.119, accuracy=0.925]\u001b[A\n",
      " 93%|██████████████████████████████████████████████▌   | 841/904 [00:09<00:00, 81.6batch/s, loss=0.118, accuracy=0.925]\u001b[A\n",
      " 94%|███████████████████████████████████████████████   | 850/904 [00:10<00:00, 81.9batch/s, loss=0.118, accuracy=0.925]\u001b[A\n",
      " 95%|███████████████████████████████████████████████▌  | 859/904 [00:10<00:00, 82.3batch/s, loss=0.118, accuracy=0.925]\u001b[A\n",
      " 96%|████████████████████████████████████████████████  | 868/904 [00:10<00:00, 82.5batch/s, loss=0.117, accuracy=0.925]\u001b[A\n",
      " 97%|████████████████████████████████████████████████▌ | 877/904 [00:10<00:00, 82.2batch/s, loss=0.117, accuracy=0.926]\u001b[A\n",
      " 98%|█████████████████████████████████████████████████ | 886/904 [00:10<00:00, 84.1batch/s, loss=0.117, accuracy=0.926]\u001b[A\n",
      " 99%|█████████████████████████████████████████████████▌| 895/904 [00:10<00:00, 85.3batch/s, loss=0.116, accuracy=0.926]\u001b[A\n",
      " 10%|▋      | 3/30 [00:50<07:00, 15.57s/epoch, loss=0.116, accuracy=0.926, val_loss=1.36, val_accuracy=0.794, lr=0.001]\u001b[A\n",
      "  0%|                                                        | 0.00/904 [00:00<?, ?batch/s, loss=0.116, accuracy=0.926]\u001b[A\n",
      "  1%|▍                                                | 9.00/904 [00:00<00:10, 83.9batch/s, loss=0.086, accuracy=0.958]\u001b[A\n",
      "  2%|▉                                                 | 18.0/904 [00:00<00:10, 82.7batch/s, loss=0.09, accuracy=0.955]\u001b[A\n",
      "  3%|█▍                                               | 27.0/904 [00:00<00:10, 82.7batch/s, loss=0.191, accuracy=0.949]\u001b[A\n",
      "  4%|█▉                                               | 36.0/904 [00:00<00:10, 83.1batch/s, loss=0.183, accuracy=0.926]\u001b[A\n",
      "  5%|██▍                                              | 45.0/904 [00:00<00:10, 83.7batch/s, loss=0.176, accuracy=0.917]\u001b[A\n",
      "  6%|██▉                                              | 54.0/904 [00:00<00:10, 81.7batch/s, loss=0.167, accuracy=0.914]\u001b[A\n",
      "  7%|███▍                                              | 63.0/904 [00:00<00:10, 80.3batch/s, loss=0.16, accuracy=0.911]\u001b[A\n",
      "  8%|███▉                                             | 72.0/904 [00:00<00:10, 81.2batch/s, loss=0.153, accuracy=0.915]\u001b[A\n",
      "  9%|████▍                                            | 81.0/904 [00:00<00:10, 81.4batch/s, loss=0.148, accuracy=0.919]\u001b[A\n",
      " 10%|████▉                                            | 90.0/904 [00:01<00:09, 82.9batch/s, loss=0.145, accuracy=0.917]\u001b[A\n",
      " 11%|█████▎                                           | 99.0/904 [00:01<00:09, 83.8batch/s, loss=0.141, accuracy=0.918]\u001b[A\n",
      " 12%|█████▉                                            | 108/904 [00:01<00:09, 84.4batch/s, loss=0.141, accuracy=0.917]\u001b[A\n",
      " 13%|██████▌                                            | 117/904 [00:01<00:09, 84.4batch/s, loss=0.138, accuracy=0.92]\u001b[A\n",
      " 14%|██████▉                                           | 126/904 [00:01<00:09, 83.4batch/s, loss=0.132, accuracy=0.922]\u001b[A\n",
      " 15%|███████▍                                          | 135/904 [00:01<00:09, 82.7batch/s, loss=0.133, accuracy=0.921]\u001b[A\n",
      " 16%|████████                                           | 144/904 [00:01<00:09, 82.9batch/s, loss=0.132, accuracy=0.92]\u001b[A\n",
      " 17%|████████▊                                           | 153/904 [00:01<00:09, 83.3batch/s, loss=0.13, accuracy=0.92]\u001b[A\n",
      " 18%|█████████▏                                         | 162/904 [00:01<00:08, 83.3batch/s, loss=0.128, accuracy=0.92]\u001b[A\n",
      " 19%|█████████▋                                         | 171/904 [00:02<00:08, 84.3batch/s, loss=0.129, accuracy=0.92]\u001b[A\n",
      " 20%|██████████▏                                        | 180/904 [00:02<00:08, 83.4batch/s, loss=0.128, accuracy=0.92]\u001b[A\n",
      " 21%|██████████▍                                       | 189/904 [00:02<00:08, 83.6batch/s, loss=0.125, accuracy=0.921]\u001b[A\n",
      " 22%|██████████▉                                       | 198/904 [00:02<00:08, 81.9batch/s, loss=0.123, accuracy=0.922]\u001b[A\n",
      " 23%|███████████▍                                      | 207/904 [00:02<00:08, 81.7batch/s, loss=0.121, accuracy=0.922]\u001b[A\n",
      " 24%|███████████▉                                      | 216/904 [00:02<00:08, 81.3batch/s, loss=0.119, accuracy=0.924]\u001b[A\n",
      " 25%|████████████▍                                     | 225/904 [00:02<00:08, 81.8batch/s, loss=0.117, accuracy=0.924]\u001b[A\n",
      " 26%|████████████▉                                     | 234/904 [00:02<00:08, 83.0batch/s, loss=0.116, accuracy=0.924]\u001b[A\n",
      " 27%|█████████████▍                                    | 243/904 [00:02<00:08, 82.1batch/s, loss=0.114, accuracy=0.925]\u001b[A\n",
      " 28%|█████████████▉                                    | 253/904 [00:03<00:07, 83.6batch/s, loss=0.114, accuracy=0.926]\u001b[A\n",
      " 29%|██████████████▍                                   | 262/904 [00:03<00:07, 80.9batch/s, loss=0.114, accuracy=0.926]\u001b[A\n",
      " 30%|██████████████▉                                   | 271/904 [00:03<00:07, 83.1batch/s, loss=0.113, accuracy=0.926]\u001b[A\n",
      " 31%|███████████████▍                                  | 280/904 [00:03<00:07, 81.9batch/s, loss=0.112, accuracy=0.926]\u001b[A\n",
      " 32%|███████████████▉                                  | 289/904 [00:03<00:07, 83.0batch/s, loss=0.111, accuracy=0.926]\u001b[A\n",
      " 33%|████████████████▍                                 | 298/904 [00:03<00:07, 84.8batch/s, loss=0.111, accuracy=0.926]\u001b[A\n",
      " 34%|████████████████▉                                 | 307/904 [00:03<00:06, 86.1batch/s, loss=0.111, accuracy=0.927]\u001b[A\n",
      " 35%|█████████████████▍                                | 316/904 [00:03<00:06, 86.4batch/s, loss=0.111, accuracy=0.926]\u001b[A\n",
      " 36%|██████████████████▎                                | 325/904 [00:03<00:06, 86.3batch/s, loss=0.11, accuracy=0.926]\u001b[A\n",
      " 37%|██████████████████▊                                | 334/904 [00:04<00:06, 85.7batch/s, loss=0.11, accuracy=0.926]\u001b[A\n",
      " 38%|███████████████████▎                               | 343/904 [00:04<00:06, 85.8batch/s, loss=0.11, accuracy=0.926]\u001b[A\n",
      " 39%|███████████████████▍                              | 352/904 [00:04<00:06, 86.1batch/s, loss=0.111, accuracy=0.925]\u001b[A\n",
      " 40%|████████████████████                              | 362/904 [00:04<00:06, 87.0batch/s, loss=0.111, accuracy=0.925]\u001b[A\n",
      " 41%|████████████████████▉                              | 371/904 [00:04<00:06, 84.9batch/s, loss=0.11, accuracy=0.925]\u001b[A\n",
      " 42%|█████████████████████▍                             | 380/904 [00:04<00:06, 85.9batch/s, loss=0.11, accuracy=0.925]\u001b[A\n",
      " 43%|█████████████████████▉                             | 389/904 [00:04<00:05, 87.0batch/s, loss=0.11, accuracy=0.925]\u001b[A\n",
      " 44%|██████████████████████▍                            | 398/904 [00:04<00:05, 87.4batch/s, loss=0.11, accuracy=0.925]\u001b[A\n",
      " 45%|██████████████████████▌                           | 407/904 [00:04<00:05, 87.8batch/s, loss=0.109, accuracy=0.926]\u001b[A\n",
      " 46%|███████████████████████                           | 416/904 [00:04<00:05, 87.6batch/s, loss=0.109, accuracy=0.926]\u001b[A\n",
      " 47%|███████████████████████▌                          | 425/904 [00:05<00:05, 87.3batch/s, loss=0.109, accuracy=0.926]\u001b[A\n",
      " 48%|████████████████████████                          | 434/904 [00:05<00:05, 83.5batch/s, loss=0.109, accuracy=0.926]\u001b[A\n",
      " 49%|████████████████████████▌                         | 443/904 [00:05<00:05, 82.9batch/s, loss=0.108, accuracy=0.926]\u001b[A\n",
      " 50%|█████████████████████████                         | 452/904 [00:05<00:05, 83.1batch/s, loss=0.108, accuracy=0.926]\u001b[A\n",
      " 51%|█████████████████████████▍                        | 461/904 [00:05<00:05, 84.2batch/s, loss=0.108, accuracy=0.926]\u001b[A\n",
      " 52%|█████████████████████████▉                        | 470/904 [00:05<00:05, 83.0batch/s, loss=0.108, accuracy=0.926]\u001b[A\n",
      " 53%|██████████████████████████▌                       | 480/904 [00:05<00:04, 85.0batch/s, loss=0.108, accuracy=0.926]\u001b[A\n",
      " 54%|███████████████████████████                       | 490/904 [00:05<00:04, 87.0batch/s, loss=0.107, accuracy=0.927]\u001b[A\n",
      " 55%|███████████████████████████▌                      | 499/904 [00:05<00:04, 86.4batch/s, loss=0.107, accuracy=0.927]\u001b[A\n",
      " 56%|████████████████████████████                      | 508/904 [00:06<00:04, 86.9batch/s, loss=0.106, accuracy=0.927]\u001b[A\n",
      " 57%|████████████████████████████▌                     | 517/904 [00:06<00:04, 84.9batch/s, loss=0.106, accuracy=0.928]\u001b[A\n",
      " 58%|█████████████████████████████                     | 526/904 [00:06<00:04, 83.8batch/s, loss=0.105, accuracy=0.928]\u001b[A\n",
      " 59%|█████████████████████████████▌                    | 535/904 [00:06<00:04, 83.5batch/s, loss=0.105, accuracy=0.928]\u001b[A\n",
      " 60%|██████████████████████████████                    | 544/904 [00:06<00:04, 83.8batch/s, loss=0.105, accuracy=0.928]\u001b[A\n",
      " 61%|██████████████████████████████▌                   | 553/904 [00:06<00:04, 82.6batch/s, loss=0.104, accuracy=0.928]\u001b[A\n",
      " 62%|███████████████████████████████                   | 562/904 [00:06<00:04, 81.5batch/s, loss=0.105, accuracy=0.928]\u001b[A\n",
      " 63%|███████████████████████████████▌                  | 571/904 [00:06<00:04, 81.1batch/s, loss=0.104, accuracy=0.929]\u001b[A\n",
      " 64%|████████████████████████████████                  | 580/904 [00:06<00:04, 80.5batch/s, loss=0.104, accuracy=0.929]\u001b[A\n",
      " 65%|████████████████████████████████▌                 | 589/904 [00:07<00:03, 79.9batch/s, loss=0.103, accuracy=0.929]\u001b[A\n",
      " 66%|█████████████████████████████████                 | 598/904 [00:07<00:03, 81.7batch/s, loss=0.103, accuracy=0.929]\u001b[A\n",
      " 67%|█████████████████████████████████▌                | 607/904 [00:07<00:03, 79.8batch/s, loss=0.102, accuracy=0.929]\u001b[A\n",
      " 68%|██████████████████████████████████                | 616/904 [00:07<00:03, 80.1batch/s, loss=0.102, accuracy=0.929]\u001b[A\n",
      " 69%|██████████████████████████████████▌               | 625/904 [00:07<00:03, 81.3batch/s, loss=0.102, accuracy=0.929]\u001b[A\n",
      " 70%|███████████████████████████████████▊               | 634/904 [00:07<00:03, 81.1batch/s, loss=0.102, accuracy=0.93]\u001b[A\n",
      " 71%|████████████████████████████████████▎              | 643/904 [00:07<00:03, 81.8batch/s, loss=0.104, accuracy=0.93]\u001b[A\n",
      " 72%|████████████████████████████████████▊              | 652/904 [00:07<00:03, 83.3batch/s, loss=0.104, accuracy=0.93]\u001b[A\n",
      " 73%|█████████████████████████████████████▎             | 661/904 [00:07<00:02, 83.4batch/s, loss=0.104, accuracy=0.93]\u001b[A\n",
      " 74%|█████████████████████████████████████▊             | 670/904 [00:08<00:02, 83.4batch/s, loss=0.103, accuracy=0.93]\u001b[A\n",
      " 75%|██████████████████████████████████████▎            | 679/904 [00:08<00:02, 83.7batch/s, loss=0.103, accuracy=0.93]\u001b[A\n",
      " 76%|██████████████████████████████████████▊            | 688/904 [00:08<00:02, 83.9batch/s, loss=0.103, accuracy=0.93]\u001b[A\n",
      " 77%|███████████████████████████████████████▎           | 697/904 [00:08<00:02, 85.0batch/s, loss=0.103, accuracy=0.93]\u001b[A\n",
      " 78%|███████████████████████████████████████▊           | 706/904 [00:08<00:02, 84.1batch/s, loss=0.103, accuracy=0.93]\u001b[A\n",
      " 79%|███████████████████████████████████████▌          | 715/904 [00:08<00:02, 85.1batch/s, loss=0.102, accuracy=0.931]\u001b[A\n",
      " 80%|████████████████████████████████████████          | 724/904 [00:08<00:02, 84.3batch/s, loss=0.102, accuracy=0.931]\u001b[A\n",
      " 81%|████████████████████████████████████████▌         | 733/904 [00:08<00:02, 83.3batch/s, loss=0.102, accuracy=0.931]\u001b[A\n",
      " 82%|█████████████████████████████████████████         | 743/904 [00:08<00:01, 85.4batch/s, loss=0.103, accuracy=0.931]\u001b[A\n",
      " 83%|█████████████████████████████████████████▌        | 752/904 [00:08<00:01, 84.0batch/s, loss=0.103, accuracy=0.931]\u001b[A\n",
      " 84%|██████████████████████████████████████████        | 761/904 [00:09<00:01, 81.5batch/s, loss=0.103, accuracy=0.931]\u001b[A\n",
      " 85%|██████████████████████████████████████████▌       | 770/904 [00:09<00:01, 83.6batch/s, loss=0.103, accuracy=0.931]\u001b[A\n",
      " 86%|███████████████████████████████████████████       | 779/904 [00:09<00:01, 83.6batch/s, loss=0.103, accuracy=0.931]\u001b[A\n",
      " 87%|███████████████████████████████████████████▌      | 788/904 [00:09<00:01, 84.9batch/s, loss=0.104, accuracy=0.931]\u001b[A\n",
      " 88%|████████████████████████████████████████████▏     | 798/904 [00:09<00:01, 87.0batch/s, loss=0.104, accuracy=0.931]\u001b[A\n",
      " 89%|████████████████████████████████████████████▋     | 807/904 [00:09<00:01, 86.1batch/s, loss=0.104, accuracy=0.931]\u001b[A\n",
      " 90%|█████████████████████████████████████████████▏    | 816/904 [00:09<00:01, 87.0batch/s, loss=0.104, accuracy=0.931]\u001b[A\n",
      " 91%|█████████████████████████████████████████████▋    | 825/904 [00:09<00:00, 87.5batch/s, loss=0.104, accuracy=0.931]\u001b[A\n",
      " 92%|██████████████████████████████████████████████▏   | 834/904 [00:09<00:00, 86.3batch/s, loss=0.104, accuracy=0.931]\u001b[A\n",
      " 93%|██████████████████████████████████████████████▋   | 843/904 [00:10<00:00, 85.6batch/s, loss=0.104, accuracy=0.931]\u001b[A\n",
      " 94%|███████████████████████████████████████████████   | 852/904 [00:10<00:00, 85.0batch/s, loss=0.104, accuracy=0.931]\u001b[A\n",
      " 95%|████████████████████████████████████████████████▌  | 861/904 [00:10<00:00, 84.0batch/s, loss=0.105, accuracy=0.93]\u001b[A\n",
      " 96%|█████████████████████████████████████████████████  | 870/904 [00:10<00:00, 84.0batch/s, loss=0.105, accuracy=0.93]\u001b[A\n",
      " 97%|█████████████████████████████████████████████████▌ | 879/904 [00:10<00:00, 82.3batch/s, loss=0.105, accuracy=0.93]\u001b[A\n",
      " 98%|██████████████████████████████████████████████████ | 888/904 [00:10<00:00, 83.7batch/s, loss=0.105, accuracy=0.93]\u001b[A\n",
      " 99%|██████████████████████████████████████████████████▌| 897/904 [00:10<00:00, 80.8batch/s, loss=0.105, accuracy=0.93]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█       | 4/30 [01:03<06:18, 14.57s/epoch, loss=0.105, accuracy=0.93, val_loss=1.36, val_accuracy=0.797, lr=0.001]\n",
      "  0%|                                                         | 0.00/904 [00:00<?, ?batch/s, loss=0.105, accuracy=0.93]\u001b[A\n",
      "  1%|▍                                               | 9.00/904 [00:00<00:11, 80.5batch/s, loss=0.0929, accuracy=0.934]\u001b[A\n",
      "  2%|▉                                                | 18.0/904 [00:00<00:10, 83.7batch/s, loss=0.096, accuracy=0.931]\u001b[A\n",
      "  3%|█▍                                              | 28.0/904 [00:00<00:10, 87.1batch/s, loss=0.0848, accuracy=0.931]\u001b[A\n",
      "  4%|██                                                 | 37.0/904 [00:00<00:10, 84.1batch/s, loss=0.1, accuracy=0.938]\u001b[A\n",
      "  5%|██▌                                              | 47.0/904 [00:00<00:09, 86.1batch/s, loss=0.091, accuracy=0.939]\u001b[A\n",
      "  6%|███                                              | 56.0/904 [00:00<00:09, 86.4batch/s, loss=0.0874, accuracy=0.94]\u001b[A\n",
      "  7%|███▍                                            | 65.0/904 [00:00<00:10, 81.6batch/s, loss=0.0882, accuracy=0.938]\u001b[A\n",
      "  8%|████                                             | 74.0/904 [00:00<00:10, 78.5batch/s, loss=0.088, accuracy=0.937]\u001b[A\n",
      "  9%|████▍                                           | 83.0/904 [00:01<00:10, 79.4batch/s, loss=0.0943, accuracy=0.934]\u001b[A\n",
      " 10%|████▉                                            | 92.0/904 [00:01<00:10, 79.8batch/s, loss=0.102, accuracy=0.929]\u001b[A\n",
      " 11%|█████▌                                            | 101/904 [00:01<00:09, 82.6batch/s, loss=0.103, accuracy=0.926]\u001b[A\n",
      " 12%|█████▉                                           | 110/904 [00:01<00:09, 83.8batch/s, loss=0.0999, accuracy=0.928]\u001b[A\n",
      " 13%|██████▋                                           | 120/904 [00:01<00:09, 86.3batch/s, loss=0.101, accuracy=0.929]\u001b[A\n",
      " 14%|███████▏                                          | 129/904 [00:01<00:09, 84.1batch/s, loss=0.0989, accuracy=0.93]\u001b[A\n",
      " 15%|███████▋                                          | 138/904 [00:01<00:09, 78.7batch/s, loss=0.0983, accuracy=0.93]\u001b[A\n",
      " 16%|███████▉                                         | 146/904 [00:01<00:09, 75.9batch/s, loss=0.0961, accuracy=0.931]\u001b[A\n",
      " 17%|████████▌                                         | 155/904 [00:01<00:09, 78.2batch/s, loss=0.0956, accuracy=0.93]\u001b[A\n",
      " 18%|█████████                                         | 164/904 [00:02<00:09, 79.3batch/s, loss=0.0946, accuracy=0.93]\u001b[A\n",
      " 19%|█████████▍                                       | 173/904 [00:02<00:09, 80.9batch/s, loss=0.0958, accuracy=0.931]\u001b[A\n",
      " 20%|█████████▊                                       | 182/904 [00:02<00:08, 82.4batch/s, loss=0.0966, accuracy=0.931]\u001b[A\n",
      " 21%|██████████▎                                      | 191/904 [00:02<00:08, 81.8batch/s, loss=0.0976, accuracy=0.932]\u001b[A\n",
      " 22%|██████████▊                                      | 200/904 [00:02<00:08, 81.9batch/s, loss=0.0965, accuracy=0.932]\u001b[A\n",
      " 23%|███████████▎                                     | 209/904 [00:02<00:08, 82.4batch/s, loss=0.0974, accuracy=0.931]\u001b[A\n",
      " 24%|███████████▊                                     | 219/904 [00:02<00:08, 85.1batch/s, loss=0.0957, accuracy=0.932]\u001b[A\n",
      " 25%|████████████▎                                    | 228/904 [00:02<00:07, 85.8batch/s, loss=0.0946, accuracy=0.933]\u001b[A\n",
      " 26%|████████████▉                                    | 238/904 [00:02<00:07, 88.6batch/s, loss=0.0961, accuracy=0.933]\u001b[A\n",
      " 27%|█████████████▍                                   | 247/904 [00:02<00:07, 88.6batch/s, loss=0.0953, accuracy=0.934]\u001b[A\n",
      " 28%|█████████████▉                                   | 257/904 [00:03<00:07, 89.4batch/s, loss=0.0949, accuracy=0.934]\u001b[A\n",
      " 30%|██████████████▍                                  | 267/904 [00:03<00:06, 91.2batch/s, loss=0.0943, accuracy=0.934]\u001b[A\n",
      " 31%|███████████████                                  | 277/904 [00:03<00:06, 92.6batch/s, loss=0.0943, accuracy=0.934]\u001b[A\n",
      " 32%|███████████████▌                                 | 287/904 [00:03<00:06, 93.4batch/s, loss=0.0935, accuracy=0.935]\u001b[A\n",
      " 33%|████████████████                                 | 297/904 [00:03<00:06, 90.7batch/s, loss=0.0932, accuracy=0.935]\u001b[A\n",
      " 34%|████████████████▋                                | 307/904 [00:03<00:06, 86.8batch/s, loss=0.0934, accuracy=0.935]\u001b[A\n",
      " 35%|█████████████████▏                               | 316/904 [00:03<00:06, 84.2batch/s, loss=0.0927, accuracy=0.935]\u001b[A\n",
      " 36%|█████████████████▌                               | 325/904 [00:03<00:07, 80.3batch/s, loss=0.0919, accuracy=0.936]\u001b[A\n",
      " 37%|██████████████████                               | 334/904 [00:03<00:07, 79.4batch/s, loss=0.0925, accuracy=0.936]\u001b[A\n",
      " 38%|██████████████████▌                              | 343/904 [00:04<00:06, 81.7batch/s, loss=0.0917, accuracy=0.936]\u001b[A\n",
      " 39%|███████████████████▍                              | 352/904 [00:04<00:06, 80.7batch/s, loss=0.092, accuracy=0.937]\u001b[A\n",
      " 40%|███████████████████▌                             | 361/904 [00:04<00:06, 80.5batch/s, loss=0.0919, accuracy=0.937]\u001b[A\n",
      " 41%|████████████████████                             | 370/904 [00:04<00:06, 81.7batch/s, loss=0.0923, accuracy=0.937]\u001b[A\n",
      " 42%|████████████████████▌                            | 379/904 [00:04<00:06, 83.3batch/s, loss=0.0917, accuracy=0.937]\u001b[A\n",
      " 43%|█████████████████████                            | 388/904 [00:04<00:06, 82.8batch/s, loss=0.0917, accuracy=0.938]\u001b[A\n",
      " 44%|█████████████████████▌                           | 397/904 [00:04<00:06, 83.8batch/s, loss=0.0909, accuracy=0.938]\u001b[A\n",
      " 45%|██████████████████████                           | 406/904 [00:04<00:05, 84.9batch/s, loss=0.0903, accuracy=0.938]\u001b[A\n",
      " 46%|███████████████████████▍                           | 415/904 [00:04<00:05, 84.8batch/s, loss=0.09, accuracy=0.939]\u001b[A\n",
      " 47%|██████████████████████▉                          | 424/904 [00:05<00:05, 85.8batch/s, loss=0.0894, accuracy=0.939]\u001b[A\n",
      " 48%|███████████████████████▍                         | 433/904 [00:05<00:05, 85.9batch/s, loss=0.0887, accuracy=0.939]\u001b[A\n",
      " 49%|███████████████████████▉                         | 442/904 [00:05<00:05, 86.0batch/s, loss=0.0884, accuracy=0.939]\u001b[A\n",
      " 50%|████████████████████████▉                         | 451/904 [00:05<00:05, 86.8batch/s, loss=0.0891, accuracy=0.94]\u001b[A\n",
      " 51%|█████████████████████████▍                        | 460/904 [00:05<00:05, 86.8batch/s, loss=0.0887, accuracy=0.94]\u001b[A\n",
      " 52%|█████████████████████████▉                        | 469/904 [00:05<00:05, 76.4batch/s, loss=0.0887, accuracy=0.94]\u001b[A\n",
      " 53%|██████████████████████████▍                       | 478/904 [00:05<00:05, 79.3batch/s, loss=0.0883, accuracy=0.94]\u001b[A\n",
      " 54%|███████████████████████████▌                       | 488/904 [00:05<00:05, 82.7batch/s, loss=0.088, accuracy=0.94]\u001b[A\n",
      " 55%|███████████████████████████▍                      | 497/904 [00:05<00:04, 84.3batch/s, loss=0.0873, accuracy=0.94]\u001b[A\n",
      " 56%|████████████████████████████                      | 507/904 [00:06<00:04, 87.9batch/s, loss=0.0873, accuracy=0.94]\u001b[A\n",
      " 57%|████████████████████████████▌                     | 517/904 [00:06<00:04, 89.3batch/s, loss=0.0873, accuracy=0.94]\u001b[A\n",
      " 58%|█████████████████████████████                     | 526/904 [00:06<00:04, 88.7batch/s, loss=0.0869, accuracy=0.94]\u001b[A\n",
      " 59%|█████████████████████████████▌                    | 535/904 [00:06<00:04, 88.2batch/s, loss=0.0866, accuracy=0.94]\u001b[A\n",
      " 60%|██████████████████████████████▏                   | 545/904 [00:06<00:04, 89.3batch/s, loss=0.0864, accuracy=0.94]\u001b[A\n",
      " 61%|██████████████████████████████                   | 555/904 [00:06<00:03, 90.3batch/s, loss=0.0857, accuracy=0.941]\u001b[A\n",
      " 62%|██████████████████████████████▋                  | 565/904 [00:06<00:03, 90.9batch/s, loss=0.0867, accuracy=0.941]\u001b[A\n",
      " 64%|███████████████████████████████▏                 | 575/904 [00:06<00:03, 89.4batch/s, loss=0.0862, accuracy=0.941]\u001b[A\n",
      " 65%|████████████████████████████████▎                 | 584/904 [00:06<00:03, 88.3batch/s, loss=0.086, accuracy=0.941]\u001b[A\n",
      " 66%|████████████████████████████████▏                | 593/904 [00:07<00:03, 88.2batch/s, loss=0.0863, accuracy=0.941]\u001b[A\n",
      " 67%|████████████████████████████████▋                | 603/904 [00:07<00:03, 90.5batch/s, loss=0.0862, accuracy=0.941]\u001b[A\n",
      " 68%|█████████████████████████████████▏               | 613/904 [00:07<00:03, 89.9batch/s, loss=0.0855, accuracy=0.942]\u001b[A\n",
      " 69%|█████████████████████████████████▋               | 622/904 [00:07<00:03, 89.3batch/s, loss=0.0852, accuracy=0.942]\u001b[A\n",
      " 70%|██████████████████████████████████▏              | 631/904 [00:07<00:03, 87.7batch/s, loss=0.0851, accuracy=0.942]\u001b[A\n",
      " 71%|██████████████████████████████████▋              | 640/904 [00:07<00:02, 88.3batch/s, loss=0.0848, accuracy=0.942]\u001b[A\n",
      " 72%|███████████████████████████████████▏             | 650/904 [00:07<00:02, 90.6batch/s, loss=0.0847, accuracy=0.942]\u001b[A\n",
      " 73%|███████████████████████████████████▊             | 660/904 [00:07<00:02, 90.4batch/s, loss=0.0843, accuracy=0.942]\u001b[A\n",
      " 74%|████████████████████████████████████▎            | 670/904 [00:07<00:02, 89.1batch/s, loss=0.0845, accuracy=0.942]\u001b[A\n",
      " 75%|████████████████████████████████████▊            | 679/904 [00:07<00:02, 89.3batch/s, loss=0.0848, accuracy=0.942]\u001b[A\n",
      " 76%|█████████████████████████████████████▎           | 688/904 [00:08<00:02, 87.5batch/s, loss=0.0849, accuracy=0.941]\u001b[A\n",
      " 77%|█████████████████████████████████████▊           | 697/904 [00:08<00:02, 84.7batch/s, loss=0.0846, accuracy=0.942]\u001b[A\n",
      " 78%|██████████████████████████████████████▎          | 706/904 [00:08<00:02, 80.5batch/s, loss=0.0845, accuracy=0.942]\u001b[A\n",
      " 79%|██████████████████████████████████████▊          | 715/904 [00:08<00:02, 78.4batch/s, loss=0.0846, accuracy=0.941]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 723/904 [00:08<00:02, 78.6batch/s, loss=0.0845, accuracy=0.942]\u001b[A\n",
      " 81%|███████████████████████████████████████▌         | 731/904 [00:08<00:02, 78.2batch/s, loss=0.0842, accuracy=0.941]\u001b[A\n",
      " 82%|████████████████████████████████████████▊         | 739/904 [00:08<00:02, 72.0batch/s, loss=0.084, accuracy=0.942]\u001b[A\n",
      " 83%|████████████████████████████████████████▍        | 747/904 [00:08<00:02, 71.0batch/s, loss=0.0838, accuracy=0.942]\u001b[A\n",
      " 84%|████████████████████████████████████████▉        | 755/904 [00:08<00:02, 72.2batch/s, loss=0.0844, accuracy=0.942]\u001b[A\n",
      " 85%|█████████████████████████████████████████▍       | 764/904 [00:09<00:01, 74.6batch/s, loss=0.0844, accuracy=0.942]\u001b[A\n",
      " 85%|█████████████████████████████████████████▊       | 772/904 [00:09<00:01, 75.5batch/s, loss=0.0844, accuracy=0.942]\u001b[A\n",
      " 86%|██████████████████████████████████████████▎      | 781/904 [00:09<00:01, 76.9batch/s, loss=0.0842, accuracy=0.942]\u001b[A\n",
      " 87%|██████████████████████████████████████████▊      | 789/904 [00:09<00:01, 75.4batch/s, loss=0.0841, accuracy=0.942]\u001b[A\n",
      " 88%|███████████████████████████████████████████▎     | 798/904 [00:09<00:01, 77.3batch/s, loss=0.0842, accuracy=0.942]\u001b[A\n",
      " 89%|███████████████████████████████████████████▋     | 807/904 [00:09<00:01, 78.7batch/s, loss=0.0839, accuracy=0.942]\u001b[A\n",
      " 90%|████████████████████████████████████████████▏    | 816/904 [00:09<00:01, 80.0batch/s, loss=0.0839, accuracy=0.941]\u001b[A\n",
      " 91%|████████████████████████████████████████████▋    | 825/904 [00:09<00:00, 81.8batch/s, loss=0.0839, accuracy=0.941]\u001b[A\n",
      " 92%|█████████████████████████████████████████████▏   | 834/904 [00:09<00:00, 82.0batch/s, loss=0.0834, accuracy=0.942]\u001b[A\n",
      " 93%|█████████████████████████████████████████████▋   | 843/904 [00:10<00:00, 83.4batch/s, loss=0.0833, accuracy=0.942]\u001b[A\n",
      " 94%|██████████████████████████████████████████████▏  | 852/904 [00:10<00:00, 84.2batch/s, loss=0.0831, accuracy=0.942]\u001b[A\n",
      " 95%|██████████████████████████████████████████████▋  | 861/904 [00:10<00:00, 84.3batch/s, loss=0.0832, accuracy=0.941]\u001b[A\n",
      " 96%|████████████████████████████████████████████████  | 870/904 [00:10<00:00, 84.4batch/s, loss=0.083, accuracy=0.942]\u001b[A\n",
      " 97%|████████████████████████████████████████████████▌ | 879/904 [00:10<00:00, 84.2batch/s, loss=0.083, accuracy=0.941]\u001b[A\n",
      " 98%|████████████████████████████████████████████████▏| 888/904 [00:10<00:00, 84.7batch/s, loss=0.0829, accuracy=0.941]\u001b[A\n",
      " 17%|▊    | 5/30 [01:17<05:51, 14.05s/epoch, loss=0.0825, accuracy=0.941, val_loss=1.52, val_accuracy=0.783, lr=0.0005]\u001b[A\n",
      "  0%|                                                       | 0.00/904 [00:00<?, ?batch/s, loss=0.0825, accuracy=0.941]\u001b[A\n",
      "  1%|▍                                               | 9.00/904 [00:00<00:11, 80.8batch/s, loss=0.0574, accuracy=0.965]\u001b[A\n",
      "  2%|▉                                               | 18.0/904 [00:00<00:10, 81.7batch/s, loss=0.0784, accuracy=0.939]\u001b[A\n",
      "  3%|█▍                                              | 27.0/904 [00:00<00:10, 81.3batch/s, loss=0.0765, accuracy=0.934]\u001b[A\n",
      "  4%|█▉                                              | 36.0/904 [00:00<00:10, 80.4batch/s, loss=0.0741, accuracy=0.934]\u001b[A\n",
      "  5%|██▍                                             | 45.0/904 [00:00<00:10, 81.3batch/s, loss=0.0811, accuracy=0.932]\u001b[A\n",
      "  6%|██▊                                             | 54.0/904 [00:00<00:10, 81.8batch/s, loss=0.0818, accuracy=0.932]\u001b[A\n",
      "  7%|███▎                                            | 63.0/904 [00:00<00:10, 82.4batch/s, loss=0.0811, accuracy=0.932]\u001b[A\n",
      "  8%|███▊                                            | 72.0/904 [00:00<00:10, 82.0batch/s, loss=0.0805, accuracy=0.932]\u001b[A\n",
      "  9%|████▎                                           | 81.0/904 [00:00<00:09, 83.7batch/s, loss=0.0803, accuracy=0.932]\u001b[A\n",
      " 10%|████▊                                           | 90.0/904 [00:01<00:09, 81.9batch/s, loss=0.0783, accuracy=0.934]\u001b[A\n",
      " 11%|█████▎                                          | 99.0/904 [00:01<00:10, 79.8batch/s, loss=0.0774, accuracy=0.934]\u001b[A\n",
      " 12%|█████▊                                           | 108/904 [00:01<00:09, 81.4batch/s, loss=0.0764, accuracy=0.935]\u001b[A\n",
      " 13%|██████▎                                          | 117/904 [00:01<00:09, 81.6batch/s, loss=0.0785, accuracy=0.935]\u001b[A\n",
      " 14%|██████▊                                          | 126/904 [00:01<00:09, 82.2batch/s, loss=0.0755, accuracy=0.938]\u001b[A\n",
      " 15%|███████▍                                          | 135/904 [00:01<00:09, 81.9batch/s, loss=0.077, accuracy=0.938]\u001b[A\n",
      " 16%|███████▊                                         | 144/904 [00:01<00:09, 79.8batch/s, loss=0.0782, accuracy=0.938]\u001b[A\n",
      " 17%|████████▎                                        | 153/904 [00:01<00:09, 80.3batch/s, loss=0.0798, accuracy=0.937]\u001b[A\n",
      " 18%|████████▊                                        | 162/904 [00:02<00:09, 77.7batch/s, loss=0.0794, accuracy=0.937]\u001b[A\n",
      " 19%|█████████▎                                       | 171/904 [00:02<00:09, 79.0batch/s, loss=0.0778, accuracy=0.938]\u001b[A\n",
      " 20%|█████████▋                                       | 179/904 [00:02<00:09, 79.2batch/s, loss=0.0778, accuracy=0.937]\u001b[A\n",
      " 21%|██████████▏                                      | 188/904 [00:02<00:08, 81.3batch/s, loss=0.0773, accuracy=0.938]\u001b[A\n",
      " 22%|██████████▋                                      | 197/904 [00:02<00:08, 79.8batch/s, loss=0.0763, accuracy=0.939]\u001b[A\n",
      " 23%|███████████▏                                     | 206/904 [00:02<00:08, 80.6batch/s, loss=0.0752, accuracy=0.941]\u001b[A\n",
      " 24%|███████████▋                                     | 215/904 [00:02<00:08, 80.0batch/s, loss=0.0749, accuracy=0.941]\u001b[A\n",
      " 25%|████████████▏                                    | 224/904 [00:02<00:08, 80.8batch/s, loss=0.0743, accuracy=0.942]\u001b[A\n",
      " 26%|████████████▋                                    | 233/904 [00:02<00:08, 82.4batch/s, loss=0.0746, accuracy=0.941]\u001b[A\n",
      " 27%|█████████████                                    | 242/904 [00:02<00:07, 83.6batch/s, loss=0.0743, accuracy=0.941]\u001b[A\n",
      " 28%|█████████████▌                                   | 251/904 [00:03<00:07, 83.9batch/s, loss=0.0735, accuracy=0.942]\u001b[A\n",
      " 29%|██████████████                                   | 260/904 [00:03<00:07, 82.6batch/s, loss=0.0734, accuracy=0.942]\u001b[A\n",
      " 30%|██████████████▌                                  | 269/904 [00:03<00:07, 83.0batch/s, loss=0.0747, accuracy=0.942]\u001b[A\n",
      " 31%|███████████████▍                                  | 278/904 [00:03<00:07, 80.7batch/s, loss=0.075, accuracy=0.942]\u001b[A\n",
      " 32%|███████████████▌                                 | 287/904 [00:03<00:07, 83.2batch/s, loss=0.0752, accuracy=0.942]\u001b[A\n",
      " 33%|████████████████▎                                 | 296/904 [00:03<00:07, 82.4batch/s, loss=0.077, accuracy=0.941]\u001b[A\n",
      " 34%|████████████████▌                                | 305/904 [00:03<00:07, 81.6batch/s, loss=0.0776, accuracy=0.941]\u001b[A\n",
      " 35%|█████████████████▎                                | 314/904 [00:03<00:07, 83.9batch/s, loss=0.0784, accuracy=0.94]\u001b[A\n",
      " 36%|██████████████████▏                                | 323/904 [00:03<00:06, 84.2batch/s, loss=0.079, accuracy=0.94]\u001b[A\n",
      " 37%|██████████████████▎                               | 332/904 [00:04<00:06, 84.9batch/s, loss=0.0785, accuracy=0.94]\u001b[A\n",
      " 38%|██████████████████▍                              | 341/904 [00:04<00:06, 83.9batch/s, loss=0.0783, accuracy=0.941]\u001b[A\n",
      " 39%|███████████████████▋                               | 350/904 [00:04<00:06, 84.8batch/s, loss=0.078, accuracy=0.94]\u001b[A\n",
      " 40%|███████████████████▍                             | 359/904 [00:04<00:06, 84.8batch/s, loss=0.0785, accuracy=0.939]\u001b[A\n",
      " 41%|████████████████████▎                             | 368/904 [00:04<00:06, 82.1batch/s, loss=0.0785, accuracy=0.94]\u001b[A\n",
      " 42%|████████████████████▍                            | 377/904 [00:04<00:06, 82.8batch/s, loss=0.0783, accuracy=0.939]\u001b[A\n",
      " 43%|█████████████████████▎                            | 386/904 [00:04<00:06, 83.2batch/s, loss=0.0778, accuracy=0.94]\u001b[A\n",
      " 44%|█████████████████████▊                            | 395/904 [00:04<00:06, 84.5batch/s, loss=0.0776, accuracy=0.94]\u001b[A\n",
      " 45%|██████████████████████▎                           | 404/904 [00:04<00:05, 84.0batch/s, loss=0.0779, accuracy=0.94]\u001b[A\n",
      " 46%|██████████████████████▍                          | 413/904 [00:05<00:05, 85.4batch/s, loss=0.0781, accuracy=0.939]\u001b[A\n",
      " 47%|███████████████████████▎                          | 422/904 [00:05<00:05, 84.8batch/s, loss=0.0776, accuracy=0.94]\u001b[A\n",
      " 48%|███████████████████████▎                         | 431/904 [00:05<00:05, 83.5batch/s, loss=0.0771, accuracy=0.941]\u001b[A\n",
      " 49%|███████████████████████▊                         | 440/904 [00:05<00:05, 82.8batch/s, loss=0.0769, accuracy=0.941]\u001b[A\n",
      " 50%|████████████████████████▎                        | 449/904 [00:05<00:05, 80.7batch/s, loss=0.0768, accuracy=0.941]\u001b[A\n",
      " 51%|████████████████████████▊                        | 458/904 [00:05<00:05, 82.1batch/s, loss=0.0765, accuracy=0.941]\u001b[A\n",
      " 52%|█████████████████████████▊                        | 467/904 [00:05<00:05, 83.8batch/s, loss=0.076, accuracy=0.941]\u001b[A\n",
      " 53%|█████████████████████████▊                       | 476/904 [00:05<00:05, 82.3batch/s, loss=0.0762, accuracy=0.941]\u001b[A\n",
      " 54%|██████████████████████████▎                      | 485/904 [00:05<00:05, 82.7batch/s, loss=0.0762, accuracy=0.941]\u001b[A\n",
      " 55%|██████████████████████████▊                      | 494/904 [00:06<00:04, 83.7batch/s, loss=0.0759, accuracy=0.941]\u001b[A\n",
      " 56%|███████████████████████████▎                     | 503/904 [00:06<00:04, 84.7batch/s, loss=0.0758, accuracy=0.941]\u001b[A\n",
      " 57%|███████████████████████████▊                     | 512/904 [00:06<00:04, 85.4batch/s, loss=0.0764, accuracy=0.941]\u001b[A\n",
      " 58%|████████████████████████████▏                    | 521/904 [00:06<00:04, 85.5batch/s, loss=0.0758, accuracy=0.941]\u001b[A\n",
      " 59%|████████████████████████████▋                    | 530/904 [00:06<00:04, 85.4batch/s, loss=0.0753, accuracy=0.941]\u001b[A\n",
      " 60%|█████████████████████████████▏                   | 539/904 [00:06<00:04, 80.1batch/s, loss=0.0758, accuracy=0.941]\u001b[A\n",
      " 61%|█████████████████████████████▋                   | 548/904 [00:06<00:04, 81.7batch/s, loss=0.0755, accuracy=0.941]\u001b[A\n",
      " 62%|██████████████████████████████▏                  | 557/904 [00:06<00:04, 82.4batch/s, loss=0.0753, accuracy=0.941]\u001b[A\n",
      " 63%|███████████████████████████████▎                  | 566/904 [00:06<00:04, 82.6batch/s, loss=0.075, accuracy=0.941]\u001b[A\n",
      " 64%|███████████████████████████████▏                 | 575/904 [00:06<00:04, 81.7batch/s, loss=0.0748, accuracy=0.942]\u001b[A\n",
      " 65%|███████████████████████████████▋                 | 584/904 [00:07<00:03, 83.7batch/s, loss=0.0745, accuracy=0.942]\u001b[A\n",
      " 66%|████████████████████████████████▏                | 593/904 [00:07<00:03, 82.5batch/s, loss=0.0743, accuracy=0.942]\u001b[A\n",
      " 67%|████████████████████████████████▋                | 602/904 [00:07<00:03, 84.0batch/s, loss=0.0739, accuracy=0.942]\u001b[A\n",
      " 68%|█████████████████████████████████                | 611/904 [00:07<00:03, 83.9batch/s, loss=0.0734, accuracy=0.943]\u001b[A\n",
      " 69%|█████████████████████████████████▋               | 621/904 [00:07<00:03, 85.9batch/s, loss=0.0742, accuracy=0.943]\u001b[A\n",
      " 70%|██████████████████████████████████▏              | 631/904 [00:07<00:03, 87.1batch/s, loss=0.0741, accuracy=0.943]\u001b[A\n",
      " 71%|██████████████████████████████████▋              | 640/904 [00:07<00:03, 86.6batch/s, loss=0.0753, accuracy=0.943]\u001b[A\n",
      " 72%|███████████████████████████████████▉              | 649/904 [00:07<00:02, 87.2batch/s, loss=0.075, accuracy=0.943]\u001b[A\n",
      " 73%|███████████████████████████████████▋             | 658/904 [00:07<00:02, 87.5batch/s, loss=0.0753, accuracy=0.943]\u001b[A\n",
      " 74%|████████████████████████████████████▏            | 667/904 [00:08<00:02, 86.4batch/s, loss=0.0754, accuracy=0.943]\u001b[A\n",
      " 75%|████████████████████████████████████▋            | 677/904 [00:08<00:02, 87.8batch/s, loss=0.0756, accuracy=0.943]\u001b[A\n",
      " 76%|█████████████████████████████████████▏           | 686/904 [00:08<00:02, 87.3batch/s, loss=0.0754, accuracy=0.943]\u001b[A\n",
      " 77%|█████████████████████████████████████▋           | 695/904 [00:08<00:02, 85.1batch/s, loss=0.0752, accuracy=0.943]\u001b[A\n",
      " 78%|██████████████████████████████████████▏          | 704/904 [00:08<00:02, 85.0batch/s, loss=0.0749, accuracy=0.943]\u001b[A\n",
      " 79%|██████████████████████████████████████▋          | 713/904 [00:08<00:02, 85.3batch/s, loss=0.0746, accuracy=0.943]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 722/904 [00:08<00:02, 86.0batch/s, loss=0.0743, accuracy=0.943]\u001b[A\n",
      " 81%|███████████████████████████████████████▌         | 731/904 [00:08<00:02, 83.9batch/s, loss=0.0743, accuracy=0.943]\u001b[A\n",
      " 82%|████████████████████████████████████████         | 740/904 [00:08<00:02, 81.5batch/s, loss=0.0743, accuracy=0.943]\u001b[A\n",
      " 83%|████████████████████████████████████████▌        | 749/904 [00:09<00:01, 82.4batch/s, loss=0.0742, accuracy=0.944]\u001b[A\n",
      " 84%|█████████████████████████████████████████        | 758/904 [00:09<00:01, 82.2batch/s, loss=0.0743, accuracy=0.944]\u001b[A\n",
      " 85%|██████████████████████████████████████████▍       | 767/904 [00:09<00:01, 82.1batch/s, loss=0.074, accuracy=0.944]\u001b[A\n",
      " 86%|██████████████████████████████████████████       | 776/904 [00:09<00:01, 80.2batch/s, loss=0.0737, accuracy=0.944]\u001b[A\n",
      " 87%|██████████████████████████████████████████▌      | 785/904 [00:09<00:01, 75.5batch/s, loss=0.0734, accuracy=0.944]\u001b[A\n",
      " 88%|██████████████████████████████████████████▉      | 793/904 [00:09<00:01, 74.7batch/s, loss=0.0731, accuracy=0.944]\u001b[A\n",
      " 89%|███████████████████████████████████████████▍     | 802/904 [00:09<00:01, 77.7batch/s, loss=0.0729, accuracy=0.944]\u001b[A\n",
      " 90%|████████████████████████████████████████████▊     | 811/904 [00:09<00:01, 80.1batch/s, loss=0.073, accuracy=0.944]\u001b[A\n",
      " 91%|████████████████████████████████████████████▌    | 821/904 [00:09<00:00, 83.9batch/s, loss=0.0728, accuracy=0.944]\u001b[A\n",
      " 92%|████████████████████████████████████████████▉    | 830/904 [00:10<00:00, 83.8batch/s, loss=0.0726, accuracy=0.945]\u001b[A\n",
      " 93%|█████████████████████████████████████████████▍   | 839/904 [00:10<00:00, 84.5batch/s, loss=0.0725, accuracy=0.945]\u001b[A\n",
      " 94%|██████████████████████████████████████████████   | 849/904 [00:10<00:00, 87.2batch/s, loss=0.0724, accuracy=0.945]\u001b[A\n",
      " 95%|██████████████████████████████████████████████▌  | 858/904 [00:10<00:00, 87.0batch/s, loss=0.0731, accuracy=0.945]\u001b[A\n",
      " 96%|███████████████████████████████████████████████▉  | 867/904 [00:10<00:00, 84.8batch/s, loss=0.073, accuracy=0.945]\u001b[A\n",
      " 97%|████████████████████████████████████████████████▍ | 876/904 [00:10<00:00, 83.0batch/s, loss=0.073, accuracy=0.945]\u001b[A\n",
      " 98%|███████████████████████████████████████████████▉ | 885/904 [00:10<00:00, 80.4batch/s, loss=0.0731, accuracy=0.944]\u001b[A\n",
      " 99%|████████████████████████████████████████████████▍| 894/904 [00:10<00:00, 82.8batch/s, loss=0.0731, accuracy=0.944]\u001b[A\n",
      " 20%|█    | 6/30 [01:30<05:28, 13.70s/epoch, loss=0.0734, accuracy=0.944, val_loss=1.47, val_accuracy=0.801, lr=0.0005]\u001b[A\n",
      " 20%|█    | 6/30 [01:30<06:03, 15.16s/epoch, loss=0.0734, accuracy=0.944, val_loss=1.47, val_accuracy=0.801, lr=0.0005]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404/404 [==============================] - 8s 6ms/step\n",
      "\n",
      "📊 Evaluation on Validation Set:\n",
      "Accuracy:  0.7864\n",
      "F1-Score:  0.4367\n",
      "Precision: 0.4279\n",
      "Recall:    0.4524\n",
      "Kappa:     0.2050\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# Compute class weights for imbalance correction\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, class_weights))\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(patience=3, factor=0.5, verbose=1),\n",
    "    TqdmCallback(verbose=1)\n",
    "]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0  # tqdm handles this\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_val = model.predict(X_val_scaled).argmax(axis=1)\n",
    "\n",
    "print(\"\\n📊 Evaluation on Validation Set:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_val, y_pred_val):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_val, y_pred_val, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_val, y_pred_val, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_val, y_pred_val, average='macro'):.4f}\")\n",
    "print(f\"Kappa:     {cohen_kappa_score(y_val, y_pred_val):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d4b58-5eb5-4974-93a0-3309f84124cc",
   "metadata": {},
   "source": [
    "### 8. Final Evaluation on the Generalization Set (10% Unseen Data)\n",
    "\n",
    "In this step, we test the model’s ability to generalize by evaluating it on a 10% holdout set that was never used during training or validation.\n",
    "\n",
    "This assessment reflects the model’s real-world robustness when facing unseen simulations and emission patterns. The same preprocessing and label encoding are applied to the generalization data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6fd8bec-6aff-4236-ae63-3b45ed867363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 1s 6ms/step\n",
      "\n",
      "📈 Generalization Set Evaluation (10% unseen data):\n",
      "Accuracy:  0.2353\n",
      "F1-Score:  0.1851\n",
      "Precision: 0.2034\n",
      "Recall:    0.2555\n",
      "Kappa:     -0.1097\n"
     ]
    }
   ],
   "source": [
    "# Prepare generalization set\n",
    "X_gen = df_gen.drop(columns=['classe', 'tag', 'Altura', 'Time'], errors='ignore')\n",
    "y_gen = df_gen['classe']\n",
    "\n",
    "# Apply label encoding to targets\n",
    "y_gen_encoded = encoder.transform(y_gen)\n",
    "\n",
    "# Generate sequences from the generalization set\n",
    "X_gen_seq, y_gen_seq, _ = create_lstm_windows(df_gen, window_size=30, step=5)\n",
    "y_gen_encoded_seq = encoder.transform(y_gen_seq)\n",
    "\n",
    "# Normalize using the same scaler (fit only on train)\n",
    "n_t, n_f = X_gen_seq.shape[1], X_gen_seq.shape[2]\n",
    "X_gen_scaled = scaler.transform(X_gen_seq.reshape(-1, n_f)).reshape(-1, n_t, n_f).astype(np.float32)\n",
    "\n",
    "# Predict\n",
    "y_gen_pred = model.predict(X_gen_scaled).argmax(axis=1)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n📈 Generalization Set Evaluation (10% unseen data):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_gen_encoded_seq, y_gen_pred):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_gen_encoded_seq, y_gen_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_gen_encoded_seq, y_gen_pred, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_gen_encoded_seq, y_gen_pred, average='macro'):.4f}\")\n",
    "print(f\"Kappa:     {cohen_kappa_score(y_gen_encoded_seq, y_gen_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32f9aae3-5d6e-4aa1-a9b9-a88a804c1f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classe\n",
      "1    81\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificar se cada tag tem uma única classe\n",
    "df_check = df.groupby(\"tag\")[\"classe\"].nunique()\n",
    "print(df_check.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "088b505d-a242-48bd-8181-e6bf938b8ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classe\n",
      "E1    81077\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"classe\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e643541a-9dec-4fe8-8316-2192dcd96d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (22952, 9901)\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f9891</th>\n",
       "      <th>f9892</th>\n",
       "      <th>f9893</th>\n",
       "      <th>f9894</th>\n",
       "      <th>f9895</th>\n",
       "      <th>f9896</th>\n",
       "      <th>f9897</th>\n",
       "      <th>f9898</th>\n",
       "      <th>f9899</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.37827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.37827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.37827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.37827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.37827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9901 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    f0   f1   f2   f3   f4   f5   f6       f7   f8   f9  ...  f9891  f9892  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.37827  0.0  0.0  ...    0.0    0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.37827  0.0  0.0  ...    0.0    0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.37827  0.0  0.0  ...    0.0    0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.37827  0.0  0.0  ...    0.0    0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.37827  0.0  0.0  ...    0.0    0.0   \n",
       "\n",
       "   f9893  f9894  f9895  f9896  f9897  f9898  f9899  classe  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0      E1  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0      E1  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0      E1  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0      E1  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0      E1  \n",
       "\n",
       "[5 rows x 9901 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "f0       0\n",
      "f6603    0\n",
      "f6596    0\n",
      "f6597    0\n",
      "f6598    0\n",
      "dtype: int64\n",
      "\n",
      "Class distribution:\n",
      "classe\n",
      "E1    7938\n",
      "E2    7938\n",
      "E3    7076\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f0</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.277011</td>\n",
       "      <td>-0.378917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.174019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.149740</td>\n",
       "      <td>-0.370363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.878439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.362990</td>\n",
       "      <td>-0.378669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.468614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f4</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>0.160716</td>\n",
       "      <td>-0.377884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.541545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.216727</td>\n",
       "      <td>-0.378913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.036876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f6</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.372791</td>\n",
       "      <td>-0.404915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.048461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f8</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.110787</td>\n",
       "      <td>-0.401333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.986943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9</th>\n",
       "      <td>22952.0</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.277125</td>\n",
       "      <td>-0.378879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.859483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean       std       min  25%  50%  75%       max\n",
       "f0  22952.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000\n",
       "f1  22952.0  0.000259  0.277011 -0.378917  0.0  0.0  0.0  3.174019\n",
       "f2  22952.0 -0.000618  0.149740 -0.370363  0.0  0.0  0.0  4.878439\n",
       "f3  22952.0 -0.000264  0.362990 -0.378669  0.0  0.0  0.0  5.468614\n",
       "f4  22952.0 -0.000453  0.160716 -0.377884  0.0  0.0  0.0  6.541545\n",
       "f5  22952.0  0.000291  0.216727 -0.378913  0.0  0.0  0.0  4.036876\n",
       "f6  22952.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000\n",
       "f7  22952.0  0.000186  0.372791 -0.404915  0.0  0.0  0.0  5.048461\n",
       "f8  22952.0  0.000008  0.110787 -0.401333  0.0  0.0  0.0  2.986943\n",
       "f9  22952.0  0.000113  0.277125 -0.378879  0.0  0.0  0.0  3.859483"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregando o dataset\n",
    "df = pd.read_csv(\"../data/processed/dataset_timeseries.csv\")\n",
    "\n",
    "# 1. Verificar formato geral\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nSample:\")\n",
    "display(df.head())\n",
    "\n",
    "# 2. Verificar se há valores nulos\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head())\n",
    "\n",
    "# 3. Verificar distribuição das classes\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df[\"classe\"].value_counts())\n",
    "\n",
    "sns.countplot(data=df, x=\"classe\")\n",
    "plt.title(\"Distribuição das classes\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Verificar variabilidade de algumas features\n",
    "sensor_sample = df.columns[:10]\n",
    "df[sensor_sample].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cada6f9d-e417-4fdf-8b9a-d6d88fea5ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with at least one non-zero value: 6810 / 9900\n",
      "Rows with at least one non-zero value: 19489 / 22952\n"
     ]
    }
   ],
   "source": [
    "# Remove coluna de classe temporariamente para análise numérica\n",
    "features_only = df.drop(columns=[\"classe\"])\n",
    "\n",
    "# 1. Quantas colunas têm pelo menos um valor diferente de zero\n",
    "non_zero_cols = (features_only != 0).any(axis=0).sum()\n",
    "print(f\"Columns with at least one non-zero value: {non_zero_cols} / {features_only.shape[1]}\")\n",
    "\n",
    "# 2. Quantas linhas têm pelo menos um valor diferente de zero\n",
    "non_zero_rows = (features_only != 0).any(axis=1).sum()\n",
    "print(f\"Rows with at least one non-zero value: {non_zero_rows} / {features_only.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680e03f-f720-42e0-b14d-487420f24d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
