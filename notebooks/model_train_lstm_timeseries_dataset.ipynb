{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b234d0b-8c94-4102-b580-69251f6e6d5c",
   "metadata": {},
   "source": [
    "# LSTM Model â€” Emission Point Classification with Optimization and Cross-Validation\n",
    "\n",
    "This notebook develops an optimized Long Short-Term Memory (LSTM) model to classify particle emission points (E1, E2, E3) based on sequential sensor readings from simulated industrial environments. Unlike previous architectures (MLP and CNN), this LSTM pipeline integrates hyperparameter tuning and cross-validation to rigorously explore model performance and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Objectives\n",
    "\n",
    "- Build a deep learning model using LSTM layers to handle time-dependent input sequences;\n",
    "- Tune model architecture and training parameters using KerasTuner;\n",
    "- Apply stratified K-fold cross-validation to ensure robust performance estimates;\n",
    "- Apply proper scientific methodology to evaluate model performance, ensuring no data leakage between simulations;\n",
    "- Evaluate final model performance on a held-out generalization set;\n",
    "- Provide a scientific comparison against CNN and MLP baselines.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Data Structure and Considerations\n",
    "\n",
    "- **Input Data**: `dataset_timeseries.csv` â€” each row represents a flattened time window of sensor data with its emission class;\n",
    "- **Reshaping**:  Features are reshaped into 3D format for LSTM (samples, time steps, sensors);\n",
    "- **Normalization**: StandardScaler is applied only to the training folds during CV, avoiding data leakage;\n",
    "- **Target Variable**: The emission class â€” `E1`, `E2`, or `E3`.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Notebook Structure\n",
    "\n",
    "1. Setup, Imports, and Random Seed Configuration;\n",
    "2. Load and Preprocess Time-Series Dataset;\n",
    "3. Prepare LSTM-Compatible Input Tensors;\n",
    "4. Stratified K-Fold Cross-Validation Setup;\n",
    "5. Define LSTM Model Builder for Tuning;\n",
    "6. Hyperparameter Optimization with Keras Tuner;\n",
    "7. Retrain Best Model on Full Training Set;\n",
    "8. Final Evaluation on Generalization Test Set;\n",
    "9. Visualization and Metric Reporting;\n",
    "10. Scientific Summary and Model Saving.\n",
    "\n",
    "---\n",
    "\n",
    "> âš ï¸ All experiments follow good machine learning practices, including stratified splitting, fold-wise normalization, and testing on unseen data for generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f5e903-bb4e-4e27-ae31-af11d8e70ca3",
   "metadata": {},
   "source": [
    "### 0. Setup, Imports, and GPU Configuration\n",
    "\n",
    "This initial cell prepares the development environment for building and tuning the LSTM model. It begins by importing all necessary libraries for data manipulation, modeling, evaluation, and hyperparameter optimization. To ensure experimental reproducibility, we fix random seeds for Python, NumPy, and TensorFlow. We also configure TensorFlow to detect and utilize the GPU, if available, while limiting GPU memory allocation to avoid resource exhaustion â€” especially important during hyperparameter tuning. This setup ensures consistency, performance, and scientific rigor for the remainder of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6f53cd-8fec-476c-b373-7a951af18d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using GPU: /physical_device:GPU:0\n",
      "TensorFlow version: 2.10.0\n",
      "NumPy version:      1.26.4\n",
      "Pandas version:     2.2.3\n"
     ]
    }
   ],
   "source": [
    "# Setup, Imports, and Random Seed Configuration\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Keras Tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# GPU Configuration (if available)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Limit memory growth instead of allocating all at once\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"âœ… Using GPU: {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ GPU configuration failed: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected â€” using CPU.\")\n",
    "\n",
    "# Versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version:      {np.__version__}\")\n",
    "print(f\"Pandas version:     {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e411c-f742-4ea7-91e0-fcbbee9f8d46",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess Time-Series Dataset\n",
    "\n",
    "In this step, we load the processed time-series dataset generated in the previous pipeline. The dataset contains flattened sensor readings organized in windows, each labeled with an emission class (E1, E2, or E3). We reshape the input features into a 3D tensor suitable for LSTM models:  \n",
    "**(samples, timesteps, sensors)**. The target labels are encoded from categorical strings to one-hot vectors for use in categorical classification. This prepares the dataset for stratified splitting and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa497b9-0097-4860-96a3-927ef4bd9c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (samples, timesteps, sensors): (22952, 30, 330)\n",
      "y shape (one-hot): (22952, 3)\n",
      "Classes: ['E1' 'E2' 'E3']\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed windowed dataset\n",
    "df = pd.read_csv('../data/processed/timeseries_dataset.csv')\n",
    "\n",
    "# Separate features (all columns except 'classe') and labels\n",
    "X_flat = df.drop(columns=['classe']).values\n",
    "y_raw  = df['classe'].values\n",
    "\n",
    "# Define window size and compute number of sensors\n",
    "WINDOW_SIZE = 30  # must match the generation script\n",
    "n_features  = X_flat.shape[1]\n",
    "n_sensors   = n_features // WINDOW_SIZE\n",
    "\n",
    "# Reshape flattened windows to 3D format: (samples, timesteps, sensors)\n",
    "X = X_flat.reshape(-1, WINDOW_SIZE, n_sensors)\n",
    "\n",
    "# Encode labels: string â†’ integer â†’ one-hot\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(y_raw)        # E1 â†’ 0, E2 â†’ 1, etc.\n",
    "y_cat = to_categorical(y_int)          # one-hot encoding\n",
    "\n",
    "# Print data shapes for confirmation\n",
    "print(\"X shape (samples, timesteps, sensors):\", X.shape)\n",
    "print(\"y shape (one-hot):\", y_cat.shape)\n",
    "print(\"Classes:\", le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded759b2-6078-4cd0-815a-8ef1e91b14ae",
   "metadata": {},
   "source": [
    "### 2. Stratified Train/Validation/Test Split\n",
    "\n",
    "To ensure robust evaluation, we split the dataset into three distinct subsets:\n",
    "- **60% for training**\n",
    "- **20% for validation**\n",
    "- **20% for final testing (generalization)**\n",
    "\n",
    "The test set is held out completely and will only be used for final evaluation after model selection and tuning.  \n",
    "Stratified splitting is applied to preserve the distribution of emission classes across all subsets, preventing bias due to class imbalance.  \n",
    "This split strategy ensures statistical integrity and prevents information leakage during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b94fbdfb-fe77-497b-9aa1-d0244cac0c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set saved at: '../data/processed/val_dataset.csv'\n",
      "Train set:      (13770, 30, 330), (13770, 3)\n",
      "Validation set: (4591, 30, 330), (4591, 3)\n",
      "Test set:       (4591, 30, 330), (4591, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1 â€” Split out the test set (20%) for generalization evaluation\n",
    "X_remain, X_test, y_remain, y_test, y_int_remain, y_int_test = train_test_split(\n",
    "    X, y_cat, y_int, test_size=0.2, random_state=SEED, stratify=y_int\n",
    ")\n",
    "\n",
    "# Step 2 â€” From the remaining 80%, split 75% for training and 25% for validation\n",
    "# This results in: 60% train, 20% val, 20% test\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_remain, y_remain, test_size=0.25, random_state=SEED, stratify=y_int_remain\n",
    ")\n",
    "\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "# Creating \"val_dataset.csv\" and saving\n",
    "df_val = pd.DataFrame(X_val_flat, columns=[f'sensor_t{t+1}_s{s+1}' for t in range(WINDOW_SIZE) for s in range(n_sensors)])\n",
    "df_val['target'] = np.argmax(y_val, axis=1)\n",
    "df_val.to_csv('../data/processed/LSTM_val_dataset.csv', index=False)\n",
    "\n",
    "# Display final shapes\n",
    "print(f\"Validation set saved at: '../data/processed/val_dataset.csv'\")\n",
    "print(f\"Train set:      {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set:       {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f178d9-f940-4903-8ae8-e9bdee77b31e",
   "metadata": {},
   "source": [
    "### 3. Stratified K-Fold Cross-Validation Setup with Normalization\n",
    "\n",
    "To robustly evaluate different LSTM configurations, we use **stratified 5-fold cross-validation**.  \n",
    "This ensures that each fold maintains a balanced distribution of emission classes.  \n",
    "\n",
    "For each fold:\n",
    "- The training and validation sets are extracted.\n",
    "- A **StandardScaler** is fitted **only on the training data** of the fold.\n",
    "- The scaler is then applied to both training and validation sets within that fold, preserving temporal dimensions.\n",
    "\n",
    "This avoids any data leakage and ensures that scaling is isolated per fold, following best practices in time series modeling and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe7cd2c-71b0-47ab-b699-0fd96d1b72aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prepared 5 stratified folds with independent normalization.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Number of folds\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Convert categorical y to integer for StratifiedKFold\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Initialize stratified K-fold\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Prepare to store folds\n",
    "folds_data = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train_int)):\n",
    "    # Split fold\n",
    "    X_fold_train = X_train[train_idx]\n",
    "    y_fold_train = y_train[train_idx]\n",
    "    \n",
    "    X_fold_val = X_train[val_idx]\n",
    "    y_fold_val = y_train[val_idx]\n",
    "    \n",
    "    # Normalize based only on training fold\n",
    "    scaler = StandardScaler()\n",
    "    n_samples, n_timesteps, n_features = X_fold_train.shape\n",
    "\n",
    "    # Flatten time and feature dims â†’ scale â†’ reshape\n",
    "    X_fold_train_flat = X_fold_train.reshape(-1, n_features)\n",
    "    X_fold_train_scaled = scaler.fit_transform(X_fold_train_flat).reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "    X_fold_val_flat = X_fold_val.reshape(-1, n_features)\n",
    "    X_fold_val_scaled = scaler.transform(X_fold_val_flat).reshape(X_fold_val.shape)\n",
    "\n",
    "    # Store for use during tuning\n",
    "    folds_data.append({\n",
    "        'X_train': X_fold_train_scaled,\n",
    "        'y_train': y_fold_train,\n",
    "        'X_val': X_fold_val_scaled,\n",
    "        'y_val': y_fold_val\n",
    "    })\n",
    "\n",
    "print(f\"âœ… Prepared {N_SPLITS} stratified folds with independent normalization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fb589-2945-436e-a587-3c7e49923689",
   "metadata": {},
   "source": [
    "### 4. Define LSTM Model Builder Function for Tuning\n",
    "\n",
    "We define a model-building function compatible with **Keras Tuner**, which will explore different combinations of hyperparameters.  \n",
    "\n",
    "The function accepts a hyperparameter object (`hp`) and returns a compiled LSTM model with the following tunable components:\n",
    "- **LSTM units**: number of recurrent units (e.g., 32 to 128)\n",
    "- **Dropout rate**: regularization to prevent overfitting\n",
    "- **Dense layer size**: number of neurons after the LSTM\n",
    "- **Learning rate**: optimizer step size (log-scaled)\n",
    "\n",
    "This design enables systematic search over a flexible LSTM architecture space, allowing us to discover the best-performing model configuration using cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537870e7-1b8e-4739-9c46-96b0bdf94bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hyperparameter: number of LSTM units\n",
    "    units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "    model.add(LSTM(units, input_shape=(WINDOW_SIZE, n_sensors)))\n",
    "\n",
    "    # Optional dropout\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Dense layer size\n",
    "    dense_units = hp.Int('dense_units', min_value=32, max_value=128, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "\n",
    "    # Learning rate\n",
    "    lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cdc50-5074-472a-a5d9-971591f0106b",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Optimization with Keras Tuner (on Fold 0)\n",
    "\n",
    "To explore optimal configurations, we use **Keras Tuner's RandomSearch** over a predefined hyperparameter space. For efficiency, tuning is performed using only the first training/validation fold (`Fold 0`). Each trial evaluates a different LSTM configuration using early stopping and validation accuracy as the objective metric. Once tuning is complete, the best hyperparameters will be used to retrain the model across all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75025c37-530e-42cc-89fa-e3660626f1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 03s]\n",
      "val_accuracy: 0.9557008147239685\n",
      "\n",
      "Best val_accuracy So Far: 0.9564270377159119\n",
      "Total elapsed time: 00h 06m 50s\n",
      "âœ… Tuning complete.\n",
      "Best Hyperparameters:\n",
      "  lstm_units: 64\n",
      "  dropout_rate: 0.30000000000000004\n",
      "  dense_units: 96\n",
      "  learning_rate: 0.000978041751919679\n"
     ]
    }
   ],
   "source": [
    "# 5. Hyperparameter Optimization using Keras Tuner (with tqdm)\n",
    "\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Use only the first fold for tuning to speed up trials\n",
    "fold0 = folds_data[0]\n",
    "\n",
    "# Tuner configuration\n",
    "tuner = RandomSearch(\n",
    "    build_lstm_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    overwrite=True,\n",
    "    directory='tuner_logs',\n",
    "    project_name='lstm_tuning'\n",
    ")\n",
    "\n",
    "# Early stopping for faster convergence\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Search with progress bar\n",
    "print(\"ðŸ” Starting hyperparameter tuning on Fold 0...\")\n",
    "\n",
    "tuner.search(\n",
    "    fold0['X_train'], fold0['y_train'],\n",
    "    validation_data=(fold0['X_val'], fold0['y_val']),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Show best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"âœ… Tuning complete.\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_hp.values.items():\n",
    "    print(f\"  {param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefeb75c-e2be-4fc5-8bfc-2e0491bd2217",
   "metadata": {},
   "source": [
    "### 6. Retrain Best LSTM Model on Full Training Set\n",
    "\n",
    "Using the best hyperparameter configuration found during tuning, we now retrain the LSTM model using the **entire training set** (i.e., train + validation). The combined set is normalized using `StandardScaler`, and a small portion (10%) is reserved internally for early stopping during training. This ensures the final model benefits from the maximum amount of labeled data while still protecting against overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77019f8-6a1b-4f26-9288-064d5b320c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "517/517 [==============================] - 8s 11ms/step - loss: 0.1712 - accuracy: 0.9304 - val_loss: 0.0994 - val_accuracy: 0.9521\n",
      "Epoch 2/50\n",
      "517/517 [==============================] - 4s 7ms/step - loss: 0.0961 - accuracy: 0.9542 - val_loss: 0.0978 - val_accuracy: 0.9521\n",
      "Epoch 3/50\n",
      "517/517 [==============================] - 4s 7ms/step - loss: 0.0931 - accuracy: 0.9556 - val_loss: 0.0977 - val_accuracy: 0.9521\n",
      "Epoch 4/50\n",
      "517/517 [==============================] - 3s 7ms/step - loss: 0.0926 - accuracy: 0.9556 - val_loss: 0.0970 - val_accuracy: 0.9521\n",
      "Epoch 5/50\n",
      "517/517 [==============================] - 4s 7ms/step - loss: 0.0921 - accuracy: 0.9558 - val_loss: 0.0983 - val_accuracy: 0.9521\n",
      "Epoch 6/50\n",
      " 31/517 [>.............................] - ETA: 3s - loss: 0.0916 - accuracy: 0.9577"
     ]
    }
   ],
   "source": [
    "# Extract best hyperparameter values\n",
    "best_model = build_lstm_model(best_hp)\n",
    "\n",
    "# Re-normalize full training + validation set\n",
    "scaler = StandardScaler()\n",
    "X_train_val_flat = X_train.reshape(-1, n_sensors)\n",
    "X_train_val_scaled = scaler.fit_transform(X_train_val_flat).reshape(X_train.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape(-1, n_sensors)\n",
    "X_val_scaled = scaler.transform(X_val_flat).reshape(X_val.shape)\n",
    "\n",
    "# Combine train + val into full training set\n",
    "X_full_train = np.concatenate([X_train_val_scaled, X_val_scaled], axis=0)\n",
    "y_full_train = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "# Final training\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_full_train, y_full_train,\n",
    "    validation_split=0.1,  # small internal validation during fit\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947bda4-0c13-401f-815a-c6d0eaa70fa1",
   "metadata": {},
   "source": [
    "### 7. Final Evaluation on the Generalization Test Set\n",
    "\n",
    "After training the final LSTM model, we evaluate its performance on the held-out **test set**, which was not used during training or tuning. We apply the same scaler used on the training data to the test set for consistency. Key classification metrics are reported, including:\n",
    "\n",
    "- Accuracy and balanced accuracy  \n",
    "- Precision, recall, and F1-score (macro-averaged)  \n",
    "- Cohenâ€™s Kappa for agreement beyond chance  \n",
    "\n",
    "A normalized confusion matrix is also plotted to visualize per-class performance. This step provides a reliable measure of the modelâ€™s real-world generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670f501-16bf-4675-8ecb-a9d7156e89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Final Evaluation on the Generalization Test Set\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalize X_test with training scaler\n",
    "X_test_flat = X_test.reshape(-1, n_sensors)\n",
    "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape)\n",
    "\n",
    "# Predict\n",
    "y_pred_probs = best_model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "macro_prec = precision_score(y_true, y_pred, average='macro')\n",
    "macro_rec = recall_score(y_true, y_pred, average='macro')\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "# Print report\n",
    "print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
    "print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
    "print(f\"Macro Precision: {macro_prec:.4f}\")\n",
    "print(f\"Macro Recall: {macro_rec:.4f}\")\n",
    "print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\", xticklabels=le.classes_, yticklabels=le.classes_, cmap=\"Blues\")\n",
    "plt.title(\"Normalized Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4809d9-7193-4277-a511-28f7f6969010",
   "metadata": {},
   "source": [
    "### Summary and Final Model Saving\n",
    "\n",
    "We summarize the LSTM modelâ€™s performance and persist the best version of the model for future inference or reproduction.  \n",
    "The saved assets include:\n",
    "\n",
    "- The trained LSTM model in Keras format (`.keras`)\n",
    "- A structured JSON file summarizing final evaluation metrics and hyperparameters\n",
    "\n",
    "These artifacts ensure reproducibility and allow the model to be easily reloaded for deployment or scientific reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cd93c-3f54-4087-8632-7ad59a581261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Summary of Results and Model Saving\n",
    "\n",
    "import json\n",
    "\n",
    "# Create results summary dictionary\n",
    "lstm_summary = {\n",
    "    \"Test Accuracy\": round(acc, 4),\n",
    "    \"Balanced Accuracy\": round(bal_acc, 4),\n",
    "    \"Macro Precision\": round(macro_prec, 4),\n",
    "    \"Macro Recall\": round(macro_rec, 4),\n",
    "    \"Macro F1-Score\": round(macro_f1, 4),\n",
    "    \"Cohen's Kappa\": round(kappa, 4),\n",
    "    \"Model Type\": \"LSTM\",\n",
    "    \"Window Size\": WINDOW_SIZE,\n",
    "    \"Sensors\": n_sensors,\n",
    "    \"Best Hyperparameters\": best_hp.values\n",
    "}\n",
    "\n",
    "# Save model\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "best_model.save(\"../models/lstm_model.keras\")\n",
    "\n",
    "# Save results summary\n",
    "with open(\"../results/final_lstm_summary.json\", \"w\") as f:\n",
    "    json.dump(lstm_summary, f, indent=4)\n",
    "\n",
    "print(\"âœ… Final LSTM model and results summary saved to ../results/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af01e5-4d9a-4ffa-b577-f31be7c794b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "# Gerar grÃ¡fico de treinamento\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history[\"loss\"], label=\"Training loss\", color=\"red\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation loss\", color=\"blue\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3244bb2-8b38-4a38-888f-c6b46500ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7027a9-52b7-4887-9303-18f9c92792a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(history.history[\"loss\"])\n",
    "train_acc = history.history[\"accuracy\"][-1]\n",
    "val_acc = history.history[\"val_accuracy\"][-1]\n",
    "\n",
    "print(f\"Ã‰pocas: {epochs}\")\n",
    "print(f\"AcurÃ¡cia de Treinamento: {train_acc:.3f}\")\n",
    "print(f\"AcurÃ¡cia de ValidaÃ§Ã£o/Teste: {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802c010-e461-4ad0-b3c0-83d8837a6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = X_train.shape[1]\n",
    "print(f\"Timesteps utilizados: {timesteps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
